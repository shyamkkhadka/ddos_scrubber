{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e193424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the protected ASes correlated with top finance and insurance companies of the top 8 countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country specific AS catgories\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# countries = pd.read_csv(\"../data/final_confirmed_customer_ases_location_all_group_2024.csv\")\n",
    "# sorted_countries = countries.sort_values(by = ['value'], ascending=False)\n",
    "# sorted_countries[0:20]\n",
    "# Top 20 countries are US, GB, AU, DE, HK, CA, SG, CH, FR, BR, IT, NL, NZ, IN, TW, JP, ID, SE, and AT.\n",
    "# No. of ASNs on those countries are 643, 96, 79, 58, 45, 42, 32, 31, 30, 27, 27, 24, 21, 20, 17, 15, 14, 14, and 13.\n",
    "\n",
    "# Define regex pattern\n",
    "pattern = r\"^final_confirmed_customer_ases_location_\\d+_2024\\.csv$\"\n",
    "directory = \"/home/shyam/jupy/ddos_scrubber/data\"\n",
    "\n",
    "country_code = \"NL\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(directory) if re.match(pattern, f)]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append it to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(directory+\"/\"+file)\n",
    "    \n",
    "    # Example string\n",
    "\n",
    "    # Use regex to extract the number between \"ases_location_\" and \"_2024\"\n",
    "    match = re.search(r'final_confirmed_customer_ases_location_(\\d+)_2024\\.csv', file)\n",
    "    scrubber_asn = match.group(1)\n",
    "    \n",
    "    # Remove rows which do not have country code\n",
    "    filtered_df = df[~df['country'].str.contains(r'^(?:\\?|.*[\\t ]\\?.*)', na=False)].copy() # Make a copy here\n",
    "    \n",
    "    # Use .loc to avoid the warning and replace spaces in 'country' column\n",
    "    filtered_df.loc[:, 'country'] = filtered_df['country'].apply(lambda x: x.replace(' ', ''))\n",
    "\n",
    "    filtered_df[['country']] = filtered_df[['country']].apply(lambda x: x.astype(str).str[0:2])\n",
    "    \n",
    "    # Remove tabs and spaces from column names\n",
    "    filtered_df.columns = filtered_df.columns.str.replace(r'[\\t ]', '', regex=True)\n",
    "    \n",
    "    ases = filtered_df[filtered_df[\"country\"] == country_code]\n",
    "    ases_copy = ases.copy()\n",
    "\n",
    "    if not ases.empty:\n",
    "        # Insert new column: scrubber ASN\n",
    "        ases_copy.loc[:, 'scrubber'] = scrubber_asn\n",
    "\n",
    "        # Put AS in front of AS numbers\n",
    "        asn = ases['asn'].astype(str)\n",
    "        \n",
    "        ases_copy['ASN'] = 'AS' + ases['asn'].astype(str)\n",
    "\n",
    "#         ases_copy.loc[:, 'ASN'] = \"AS\"+str(asn)\n",
    "        dataframes.append(ases_copy)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes)\n",
    "\n",
    "asdb = pd.read_csv(\"/home/shyam/jupy/ddos_scrubber/data/2024-01_categorized_ases.csv\", low_memory=False)\n",
    "asdb_selected = asdb[[\"ASN\", \"Category 1 - Layer 1\", \"Category 1 - Layer 2\"]]\n",
    "\n",
    "# Merge our data with ASdb \n",
    "merged_df = combined_df.merge(asdb_selected, how='inner', on='ASN')\n",
    "# Save the results to a new file (optional)\n",
    "output_file = \"../data/protected_ases_\"+country_code+\".csv\"\n",
    "merged_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e19162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final_confirmed_customer_ases_location_all_group__01_dec_2024.csv\n",
      "          country  asn_count\n",
      "95   US                  676\n",
      "30   GB                  100\n",
      "7    AU                   82\n",
      "0    ?                    61\n",
      "22   DE                   58\n",
      "36   HK                   51\n",
      "14   CA                   50\n",
      "29   FR                   33\n",
      "15   CH                   31\n"
     ]
    }
   ],
   "source": [
    "# Country specific AS catgories\n",
    "# After TMA\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Group by countries\n",
    "df = pd.read_csv(\"../data/after_tma/final_confirmed_customer_ases_location_01_dec_2024.csv\")\n",
    "\n",
    "# Group by org_country and count unique ASNs\n",
    "asn_counts = df.groupby(\"org_country\")[\"asn\"].nunique().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "asn_counts.columns = [\"country\", \"asn_count\"]\n",
    "\n",
    "# Sort by count descending (optional)\n",
    "asn_counts = asn_counts.sort_values(by=\"asn_count\", ascending=False)\n",
    "\n",
    "# Save to a new CSV\n",
    "asn_counts.to_csv(\"final_confirmed_customer_ases_location_all_group__01_dec_2024.csv\", index=False)\n",
    "\n",
    "print(\"Saved final_confirmed_customer_ases_location_all_group__01_dec_2024.csv\")\n",
    "sorted_countries = asn_counts.sort_values(by = ['asn_count'], ascending=False)\n",
    "print(sorted_countries[0:9])\n",
    "\n",
    "# countries = pd.read_csv(\"../data/final_confirmed_customer_ases_location_all_group_2024.csv\")\n",
    "# sorted_countries = countries.sort_values(by = ['value'], ascending=False)\n",
    "# sorted_countries[0:20]\n",
    "# Top 8 countries are US, GB, AU, DE, HK, CA, SG, CH, FR.\n",
    "# No. of ASNs on those countries are 676, 100, 82, 58, 51, 50, 33, 31.\n",
    "\n",
    "# Define regex pattern\n",
    "directory = \"/home/shyam/jupy/ddos_scrubber/data/after_tma/\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "country_code = \"US\"\n",
    "\n",
    "df = pd.read_csv(directory + \"final_confirmed_customer_ases_location_01_dec_2024.csv\")\n",
    "\n",
    "# Remove rows which do not have country code\n",
    "filtered_df = df[~df['org_country'].str.contains(r'^(?:\\?|.*[\\t ]\\?.*)', na=False)].copy() # Make a copy here\n",
    "\n",
    "# Use .loc to avoid the warning and replace spaces in 'org_country' column\n",
    "filtered_df.loc[:, 'org_country'] = filtered_df['org_country'].apply(lambda x: x.replace(' ', ''))\n",
    "\n",
    "filtered_df[['org_country']] = filtered_df[['org_country']].apply(lambda x: x.astype(str).str[0:2])\n",
    "\n",
    "# Remove tabs and spaces from column names\n",
    "filtered_df.columns = filtered_df.columns.str.replace(r'[\\t ]', '', regex=True)\n",
    "\n",
    "ases = filtered_df[filtered_df[\"org_country\"] == country_code]\n",
    "ases_copy = ases.copy()\n",
    "\n",
    "if not ases.empty:\n",
    "    # Put AS in front of AS numbers\n",
    "    asn = ases['asn'].astype(str)\n",
    "    ases_copy['ASN'] = 'AS' + ases['asn'].astype(str)\n",
    "    dataframes.append(ases_copy)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes)\n",
    "\n",
    "asdb = pd.read_csv(\"/home/shyam/jupy/ddos_scrubber/data/2024-01_categorized_ases.csv\", low_memory=False)\n",
    "asdb_selected = asdb[[\"ASN\", \"Category 1 - Layer 1\", \"Category 1 - Layer 2\"]]\n",
    "\n",
    "# Merge our data with ASdb \n",
    "merged_df = combined_df.merge(asdb_selected, how='inner', on='ASN')\n",
    "# Save the results to a new file (optional)\n",
    "output_file = \"../data/after_tma/protected_ases_\"+country_code+\".csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# See if the finance and insurance type of ASNs of the US covers the top banks of the US\n",
    "fin_insurances = pd.read_csv(\"../data/after_tma/protected_ases_\"+country_code+\".csv\")\n",
    "fin_insurances = fin_insurances.loc[fin_insurances[\"Category 1 - Layer 1\"] == \"Finance and Insurance\"]\n",
    "fin_insurances[[\"org_name\", \"asn\"]].to_csv(\"../data/after_tma/protected_ases_banks_\"+country_code+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e06ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the ISP type of ASNs of the US covers the top banks of the US\n",
    "country_code = \"US\"\n",
    "import pandas as pd\n",
    "fin_insurances = pd.read_csv(\"../data/protected_ases_\"+country_code+\".csv\")\n",
    "fin_insurances = fin_insurances.loc[fin_insurances[\"Category 1 - Layer 2\"] == \"Internet Service Provider (ISP)\"]\n",
    "fin_insurances[[\"org_name\", \"asn\"]].to_csv(\"../data/protected_ases_isps_\"+country_code+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf182c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the protected ases of individual scrubbers and save it into a file\n",
    "# customers_ases_scrubber_scrubber_asn_year.csv\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# scrubber_asns = [\"32787\", \"13335\", \"19551\", \"198949\", \"19905\"]\n",
    "year = \"2024\"\n",
    "mon = \"jan\"\n",
    "\n",
    "path = f\"/home/shyam/jupy/ddos_scrubber/data/{year}/\"\n",
    "\n",
    "all_record_ases = []  # Stores number of customer ases for a month\n",
    "\n",
    "for scrubber_asn in scrubber_asns:\n",
    "    single_record_ases = []  # Stores number of customer ases for a month\n",
    "\n",
    "    confirmed_customers1 = pd.read_csv(path + \"confirmed_customers_as\" + scrubber_asn + \"_\" + year + \".csv\")\n",
    "    confirmed_customers1_unique_origin_ases = confirmed_customers1['origin_as'].unique()\n",
    "\n",
    "    if scrubber_asn != \"13335\":  # Cloudflare does not have path prepending case\n",
    "        path_prepending = pd.read_csv(\n",
    "            path + \"unique_optimized_provider_as\" + scrubber_asn + \"_path_prepend_01_\"+mon+\"_\" + year + \".csv\")\n",
    "        path_prepending_unique_origin_ases = path_prepending['origin_as'].unique()\n",
    "        confirmed_customers2_ases = list(\n",
    "            set(path_prepending_unique_origin_ases) - set(confirmed_customers1_unique_origin_ases))\n",
    "    else:\n",
    "        confirmed_customers2_ases = []\n",
    "\n",
    "    if scrubber_asn != \"13335\":  # Cloudflare does not have path prepending case\n",
    "        df = pd.read_csv(path + \"unique_optimized_provider_not_as\" + scrubber_asn + \"_01_\"+mon+\"_\" + year + \"_v3.csv\")\n",
    "        sibling_path = df.loc[(df['siblings'] == 1) & (df['new_provider_sibling_check'] == int(scrubber_asn))]\n",
    "        sibling_path_unique_origin_ases = sibling_path['origin_as'].unique()\n",
    "    else:\n",
    "        sibling_path_unique_origin_ases = []\n",
    "\n",
    "    confirmed_customers3_ases = list(\n",
    "        set(sibling_path_unique_origin_ases) - set(confirmed_customers2_ases) - set(\n",
    "            confirmed_customers1_unique_origin_ases))\n",
    "\n",
    "    confirmed_customers_ases = list(confirmed_customers1_unique_origin_ases) + list(\n",
    "        confirmed_customers2_ases) + list(confirmed_customers3_ases)\n",
    "\n",
    "    # Save to CSV file\n",
    "    with open(\"/home/shyam/jupy/data/customers_ases_scrubber_\" + scrubber_asn + \"_\"+mon+\"_\"+year+\".csv\", \"w\", newline=\"\") as file:\n",
    "        #     with open(\"../data/customers_ases_scrubber_\"+scrubber_asn+\"_2024_cloudflare.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"as\"])  # Write header\n",
    "        for ases in confirmed_customers_ases:\n",
    "            writer.writerow([ases])  # Write each name in a new row\n",
    "    print(\"Done for scrubber \", scrubber_asn)\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aruba: Find all the protected ases of individual scrubbers and save it into a file\n",
    "# customers_ases_scrubber_scrubber_asn_mon_year.csv\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "year = \"2020\"\n",
    "months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "# List your scrubber ASNs here\n",
    "scrubber_asns = [\"32787\", \"13335\", \"19551\", \"198949\", \"19905\"] \n",
    "\n",
    "for mon in months:\n",
    "    print(f\"Processing month: {mon}\")\n",
    "    \n",
    "    for scrubber_asn in scrubber_asns:\n",
    "        path = \"/data/shared_dir/ddos_scrubber/as\"+scrubber_asn+\"/\"+year+\"/\"\n",
    "        all_record_ases = []  # Stores number of customer ases for a month\n",
    "\n",
    "        # Load confirmed customers\n",
    "        confirmed_customers1 = pd.read_csv(path + f\"confirmed_customers_as{scrubber_asn}_01_{mon}_{year}.csv\")\n",
    "        confirmed_customers1_ases = confirmed_customers1['origin_as'].unique()\n",
    "        \n",
    "        if os.path.exists(path + \"unique_optimized_provider_as\" + scrubber_asn + \"_path_prepend_01_\" + mon + \"_\" + year + \".csv\"):\n",
    "            path_prepending = pd.read_csv(path + \"unique_optimized_provider_as\" + scrubber_asn + \"_path_prepend_01_\" + mon + \"_\" + year + \".csv\")\n",
    "            path_prepending_unique_origin_ases = path_prepending['origin_as'].unique()\n",
    "            confirmed_customers2_ases = path_prepending_unique_origin_ases\n",
    "        else:\n",
    "            confirmed_customers2_ases = []\n",
    "            \n",
    "        if os.path.exists(path + \"unique_optimized_provider_not_as\" + scrubber_asn + \"_01_\" + mon + \"_\" + year + \"_v3.csv\"):\n",
    "            df = pd.read_csv(\n",
    "                path + \"unique_optimized_provider_not_as\" + scrubber_asn + \"_01_\" + mon + \"_\" + year + \"_v3.csv\")\n",
    "            sibling_path = df.loc[(df['siblings'] == 1) & (df['new_provider_sibling_check'] == int(scrubber_asn))]\n",
    "            sibling_path_unique_origin_ases = sibling_path['origin_as'].unique()\n",
    "            confirmed_customers3_ases = sibling_path_unique_origin_ases\n",
    "\n",
    "        else:\n",
    "            confirmed_customers3_ases = []\n",
    "\n",
    "        # Combine all unique origin ASes\n",
    "        confirmed_customers_ases = set(confirmed_customers1_ases) | set(confirmed_customers2_ases) | set(confirmed_customers3_ases)  # Union\n",
    "\n",
    "        print(f\"Customers 1: {len(confirmed_customers1_ases)}, 2: {len(confirmed_customers2_ases)}, 3: {len(confirmed_customers3_ases)}, final {len(confirmed_customers_ases)}\")\n",
    "        # Write to CSV\n",
    "        output_path = f\"{path}customers_ases_scrubber_{scrubber_asn}_{mon}_{year}.csv\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        with open(output_path, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"as\"])  # Header\n",
    "            for ases in confirmed_customers_ases:\n",
    "                writer.writerow([ases])\n",
    "\n",
    "        print(f\"Done for scrubber {scrubber_asn} in {mon}\")\n",
    "print(\"Completed all months.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ab2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total protected ases of all scrubbers\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "year = \"2017\"\n",
    " \n",
    "pattern = r\"^customers_ases_scrubber_\\d+_\"+year+\"\\.csv$\" #customers_ases_scrubber_19905_2023.csv\n",
    "directory = \"/home/shyam/jupy/ddos_scrubber/data\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(directory) if re.match(pattern, f)]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append it to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(directory+\"/\"+file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Merge all dataframes into one\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe into a new CSV file\n",
    "merged_df.to_csv(\"../data/customers_ases_scrubber_all_\"+year+\".csv\", index=False) \n",
    "\n",
    "merged_df_unique = merged_df.drop_duplicates()\n",
    "\n",
    "# Save the merged dataframe into a new CSV file\n",
    "merged_df_unique.to_csv(\"../data/customers_ases_unique_scrubber_all_\"+year+\".csv\", index=False) \n",
    "print(len(merged_df_unique))\n",
    "\n",
    "print(\"Completed. All the ASes of %s saved in file customers_ases_scrubber_all_%s.csv\"  %(year, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total protected ases of all scrubbers monthwise\n",
    "# After TMA\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "years = [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
    "day = \"01\"\n",
    "months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "# List your scrubber ASNs here\n",
    "scrubber_asns = [\"32787\", \"13335\", \"19551\", \"198949\", \"19905\"]\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    for mon in months:\n",
    "        print(f\"ðŸ”„ Processing month: {mon}\")\n",
    "\n",
    "        all_matching_files = []\n",
    "\n",
    "        for scrubber in scrubber_asns:\n",
    "            base_dir = f\"../data/after_tma/as{scrubber}\"\n",
    "\n",
    "            # Regex to match files like customers_ases_scrubber_13335_01_jan_2024.csv\n",
    "            pattern = rf\"^customers_ases_scrubber_\\d+_{day}_{mon}_{year}\\.csv$\"\n",
    "\n",
    "            for root, dirs, files in os.walk(base_dir):\n",
    "                for file in files:\n",
    "                    if re.match(pattern, file):\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        all_matching_files.append(full_path)\n",
    "\n",
    "        # Merge all matching files\n",
    "        if all_matching_files:\n",
    "            dataframes = [pd.read_csv(f) for f in all_matching_files]\n",
    "            merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "            print(f\"âœ… Without removing duplicates: {len(merged_df)}\")\n",
    "            # Optional: Drop duplicates\n",
    "            merged_df_unique = merged_df.drop_duplicates()\n",
    "\n",
    "            # Output path\n",
    "            output_file = f\"../data/after_tma/customers_ases_scrubber_all_{day}_{mon}_{year}.csv\"\n",
    "            merged_df_unique.to_csv(output_file, index=False)\n",
    "            print(f\"ðŸ“ Saved merged file: {output_file} ({len(merged_df_unique)} unique rows)\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ No files found for {mon} {year}\")\n",
    "\n",
    "    print(f\"âœ… Done for all months for year .{year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490ccf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get AS rank of all ASes from CAIDA AS rank API\n",
    "# This part of the code is used from https://github.com/bgpkit/pyasrank\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "ASRANK_ENDPOINT = \"https://api.asrank.caida.org/v2/graphql\"\n",
    "\n",
    "\n",
    "def ts_to_date_str(ts):\n",
    "    \"\"\"\n",
    "    Convert timestamp to a date. This is used for ASRank API which only takes\n",
    "    date strings with no time as parameters.api\n",
    "    \"\"\"\n",
    "    return datetime.utcfromtimestamp(int(ts)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "class AsRank:\n",
    "    \"\"\"\n",
    "    Utilities for using ASRank services\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_ts=\"\"):\n",
    "        self.data_ts = None\n",
    "\n",
    "        # various caches to avoid duplicate queries\n",
    "        self.cache = None\n",
    "        self.cone_cache = None\n",
    "        self.neighbors_cache = None\n",
    "        self.siblings_cache = None\n",
    "        self.organization_cache = None\n",
    "\n",
    "        self.queries_sent = 0\n",
    "\n",
    "        self.session = None\n",
    "        self._initialize_session()\n",
    "\n",
    "        self.init_cache(max_ts)\n",
    "\n",
    "    def _initialize_session(self):\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=1,\n",
    "                        status_forcelist=[500, 502, 503, 504])\n",
    "        self.session.mount(ASRANK_ENDPOINT, HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    def _close_session(self):\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "\n",
    "    def _send_request(self, query):\n",
    "        \"\"\"\n",
    "        send requests to ASRank endpoint\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        r = self.session.post(url=ASRANK_ENDPOINT, json={'query': query})\n",
    "        r.raise_for_status()\n",
    "        self.queries_sent += 1\n",
    "        return r\n",
    "\n",
    "    def init_cache(self, ts):\n",
    "        \"\"\"\n",
    "        Initialize the ASRank cache for the timestamp ts\n",
    "        :param ts:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.cone_cache = {}\n",
    "        self.neighbors_cache = {}\n",
    "        self.siblings_cache = {}\n",
    "        self.organization_cache = {}\n",
    "        self.queries_sent = 0\n",
    "        if isinstance(ts, int):\n",
    "            ts = ts_to_date_str(ts)\n",
    "\n",
    "        ####\n",
    "        # Try to cache datasets available before the given ts\n",
    "        ####\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              datasets(dateStart:\"2000-01-01\", dateEnd:\"%s\", sort:\"-date\", first:1){\n",
    "                edges {\n",
    "                  node {\n",
    "                    date\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % ts\n",
    "        r = self._send_request(graphql_query)\n",
    "\n",
    "        edges = r.json()['data']['datasets']['edges']\n",
    "        if edges:\n",
    "            self.data_ts = edges[0][\"node\"][\"date\"]\n",
    "            return\n",
    "\n",
    "        # if code reaches here, we have not found any datasets before ts. we should now try to find one after ts.\n",
    "        # this is the best effort results\n",
    "        logging.warning(\"cannot find dataset before date %s, looking for the closest one after it now\" % ts)\n",
    "\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              datasets(dateStart:\"%s\", sort:\"date\", first:1){\n",
    "                edges {\n",
    "                  node {\n",
    "                    date\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % ts\n",
    "        r = self._send_request(graphql_query)\n",
    "        edges = r.json()['data']['datasets']['edges']\n",
    "        if edges:\n",
    "            self.data_ts = edges[0][\"node\"][\"date\"]\n",
    "            logging.warning(\"found closest dataset date to be %s\" % self.data_ts)\n",
    "            return\n",
    "        else:\n",
    "            raise ValueError(\"no datasets from ASRank available to use for tagging\")\n",
    "\n",
    "    def _query_asrank_for_asns(self, asns, chunk_size=100):\n",
    "        asns = [str(asn) for asn in asns]\n",
    "        asns_needed = [asn for asn in asns if asn not in self.cache]\n",
    "        if not asns_needed:\n",
    "            return\n",
    "\n",
    "        # https://stackoverflow.com/a/312464/768793\n",
    "        def chunks(lst, n):\n",
    "            \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "            for i in range(0, len(lst), n):\n",
    "                yield lst[i:i + n]\n",
    "\n",
    "        for asns in chunks(asns_needed, chunk_size):\n",
    "\n",
    "            graphql_query = \"\"\"\n",
    "                {\n",
    "                  asns(asns: %s, dateStart: \"%s\", dateEnd: \"%s\", first:%d, sort:\"-date\") {\n",
    "                    edges {\n",
    "                      node {\n",
    "                        date\n",
    "                        asn\n",
    "                        asnName\n",
    "                        rank\n",
    "                        organization{\n",
    "                          country{\n",
    "                            iso\n",
    "                            name\n",
    "                          }\n",
    "                          orgName\n",
    "                          orgId\n",
    "                        } asnDegree {\n",
    "                          provider\n",
    "                          peer\n",
    "                          customer\n",
    "                          total\n",
    "                          transit\n",
    "                          sibling\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "            \"\"\" % (json.dumps(asns), self.data_ts, self.data_ts, len(asns))\n",
    "            r = self._send_request(graphql_query)\n",
    "            try:\n",
    "                for node in r.json()['data']['asns']['edges']:\n",
    "                    data = node['node']\n",
    "                    if data['asn'] not in self.cache:\n",
    "                        if \"asnDegree\" in data:\n",
    "                            degree = data[\"asnDegree\"]\n",
    "                            degree[\"provider\"] = degree[\"provider\"] or 0\n",
    "                            degree[\"customer\"] = degree[\"customer\"] or 0\n",
    "                            degree[\"peer\"] = degree[\"peer\"] or 0\n",
    "                            degree[\"sibling\"] = degree[\"sibling\"] or 0\n",
    "                            data[\"asnDegree\"] = degree\n",
    "                        self.cache[data['asn']] = data\n",
    "                for asn in asns:\n",
    "                    if asn not in self.cache:\n",
    "                        self.cache[asn] = None\n",
    "            except KeyError as e:\n",
    "                logging.error(\"Error in node: {}\".format(r.json()))\n",
    "                logging.error(\"Request: {}\".format(graphql_query))\n",
    "                raise e\n",
    "\n",
    "   \n",
    "    ###########\n",
    "    # AS_RANK #\n",
    "    ###########\n",
    "\n",
    "    def get_degree(self, asn):\n",
    "        \"\"\"\n",
    "        Get relationship summary for asn, including number of customers, providers, peers, etc.\n",
    "\n",
    "        Example return dictionary:\n",
    "        {\n",
    "            \"provider\": 0,\n",
    "            \"peer\": 31,\n",
    "            \"customer\": 1355,\n",
    "            \"total\": 1386,\n",
    "            \"transit\": 1318,\n",
    "            \"sibling\": 25\n",
    "        }\n",
    "        :param asn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn])\n",
    "        if self.cache[asn] is None:\n",
    "            return None\n",
    "\n",
    "        return self.cache[asn][\"asnDegree\"]\n",
    "\n",
    "      \n",
    "    def cache_asrank_chunk(self, asns: list, chunk_size: int):\n",
    "        \"\"\"\n",
    "        Query asrank info in chunk to boost individual asrank queries performance later.\n",
    "\n",
    "        :param asns:\n",
    "        :param chunk_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns(asns, chunk_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd363bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find relationship between ASes on an AS Path\n",
    "as_rank = AsRank(\"2024-01-01\")\n",
    "a = as_rank.get_degree(\"23752\")[\"customer\"]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c681c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = a[\"customer\"]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many of them are stub\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/customers_ases_unique_scrubber_all_2024.csv\")\n",
    "ases = df[\"as\"]\n",
    "\n",
    "transit_counter = 0\n",
    "for asn in ases:\n",
    "    # Check customer AS using ASRank\n",
    "    as_rank = AsRank(\"2024-01-01\")\n",
    "    asn_degree = as_rank.get_degree(str(asn))\n",
    "    customer_count = asn_degree[\"customer\"]\n",
    "    if customer_count > 0:\n",
    "        stub_counter += 1\n",
    "        print(f\"Degree of AS{asn} is {asn_degree}.\")\n",
    "print(transit_counter)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3269989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of AS3 is {'provider': 11, 'peer': 3, 'customer': 2, 'total': 16, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS28690 is {'provider': 4, 'peer': 0, 'customer': 2, 'total': 6, 'transit': 6, 'sibling': 1}.\n",
      "Degree of AS12357 is {'provider': 2, 'peer': 1, 'customer': 95, 'total': 98, 'transit': 96, 'sibling': 1}.\n",
      "Degree of AS4199 is {'provider': 3, 'peer': 0, 'customer': 2, 'total': 5, 'transit': 4, 'sibling': 1}.\n",
      "Degree of AS6303 is {'provider': 3, 'peer': 0, 'customer': 2, 'total': 5, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS24756 is {'provider': 1, 'peer': 0, 'customer': 1, 'total': 2, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS47388 is {'provider': 8, 'peer': 0, 'customer': 1, 'total': 9, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS43338 is {'provider': 5, 'peer': 2, 'customer': 2, 'total': 9, 'transit': 9, 'sibling': 1}.\n",
      "Degree of AS2386 is {'provider': 5, 'peer': 1, 'customer': 9, 'total': 15, 'transit': 10, 'sibling': 0}.\n",
      "Degree of AS14733 is {'provider': 4, 'peer': 0, 'customer': 1, 'total': 5, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS20940 is {'provider': 105, 'peer': 354, 'customer': 14, 'total': 473, 'transit': 438, 'sibling': 7}.\n",
      "Degree of AS2516 is {'provider': 9, 'peer': 125, 'customer': 173, 'total': 307, 'transit': 307, 'sibling': 0}.\n",
      "Degree of AS25055 is {'provider': 1, 'peer': 0, 'customer': 1, 'total': 2, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS53741 is {'provider': 5, 'peer': 0, 'customer': 1, 'total': 6, 'transit': 3, 'sibling': 1}.\n",
      "Degree of AS10794 is {'provider': 21, 'peer': 1, 'customer': 1, 'total': 23, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS4680 is {'provider': 4, 'peer': 15, 'customer': 1, 'total': 20, 'transit': 7, 'sibling': 0}.\n",
      "Degree of AS25273 is {'provider': 4, 'peer': 9, 'customer': 2, 'total': 15, 'transit': 10, 'sibling': 0}.\n",
      "Degree of AS9021 is {'provider': 3, 'peer': 0, 'customer': 7, 'total': 10, 'transit': 9, 'sibling': 0}.\n",
      "Degree of AS13169 is {'provider': 1, 'peer': 0, 'customer': 1, 'total': 2, 'transit': 2, 'sibling': 1}.\n",
      "Degree of AS132029 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 2}.\n",
      "Degree of AS9166 is {'provider': 2, 'peer': 1, 'customer': 1, 'total': 4, 'transit': 3, 'sibling': 1}.\n",
      "Degree of AS27715 is {'provider': 8, 'peer': 27, 'customer': 5, 'total': 40, 'transit': 36, 'sibling': 2}.\n",
      "Degree of AS9304 is {'provider': 16, 'peer': 1006, 'customer': 215, 'total': 1237, 'transit': 685, 'sibling': 2}.\n",
      "Degree of AS3269 is {'provider': 2, 'peer': 25, 'customer': 179, 'total': 206, 'transit': 205, 'sibling': 1}.\n",
      "Degree of AS15580 is {'provider': 4, 'peer': 28, 'customer': 1, 'total': 33, 'transit': 2, 'sibling': 1}.\n",
      "Degree of AS58627 is {'provider': 5, 'peer': 20, 'customer': 1, 'total': 26, 'transit': 18, 'sibling': 0}.\n",
      "Degree of AS17685 is {'provider': 6, 'peer': 13, 'customer': 1, 'total': 20, 'transit': 20, 'sibling': 1}.\n",
      "Degree of AS40260 is {'provider': 1, 'peer': 0, 'customer': 1, 'total': 2, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS9541 is {'provider': 9, 'peer': 193, 'customer': 17, 'total': 219, 'transit': 212, 'sibling': 1}.\n",
      "Degree of AS7532 is {'provider': 2, 'peer': 1, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS26034 is {'provider': 4, 'peer': 0, 'customer': 1, 'total': 5, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS11696 is {'provider': 4, 'peer': 1, 'customer': 3, 'total': 8, 'transit': 6, 'sibling': 0}.\n",
      "Degree of AS36351 is {'provider': 19, 'peer': 257, 'customer': 15, 'total': 291, 'transit': 276, 'sibling': 0}.\n",
      "Degree of AS34358 is {'provider': 6, 'peer': 0, 'customer': 3, 'total': 9, 'transit': 7, 'sibling': 1}.\n",
      "Degree of AS11840 is {'provider': 4, 'peer': 0, 'customer': 1, 'total': 5, 'transit': 3, 'sibling': 1}.\n",
      "Degree of AS34397 is {'provider': 5, 'peer': 0, 'customer': 5, 'total': 10, 'transit': 8, 'sibling': 1}.\n",
      "Degree of AS14201 is {'provider': 10, 'peer': 1, 'customer': 1, 'total': 12, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS12200 is {'provider': 8, 'peer': 128, 'customer': 17, 'total': 153, 'transit': 151, 'sibling': 3}.\n",
      "Degree of AS204732 is {'provider': 2, 'peer': 0, 'customer': 2, 'total': 4, 'transit': 4, 'sibling': 0}.\n",
      "Degree of AS26577 is {'provider': 5, 'peer': 1, 'customer': 1, 'total': 7, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS16406 is {'provider': 5, 'peer': 5, 'customer': 1, 'total': 11, 'transit': 5, 'sibling': 1}.\n",
      "Degree of AS206977 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS10397 is {'provider': 4, 'peer': 1, 'customer': 1, 'total': 6, 'transit': 3, 'sibling': 2}.\n",
      "Degree of AS133448 is {'provider': 7, 'peer': 1, 'customer': 2, 'total': 10, 'transit': 6, 'sibling': 0}.\n",
      "Degree of AS328041 is {'provider': 6, 'peer': 26, 'customer': 1, 'total': 33, 'transit': 17, 'sibling': 0}.\n",
      "Degree of AS51561 is {'provider': 5, 'peer': 92, 'customer': 5, 'total': 102, 'transit': 97, 'sibling': 0}.\n",
      "Degree of AS14717 is {'provider': 5, 'peer': 10, 'customer': 13, 'total': 28, 'transit': 26, 'sibling': 0}.\n",
      "Degree of AS139646 is {'provider': 8, 'peer': 1, 'customer': 1, 'total': 10, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS33164 is {'provider': 5, 'peer': 2, 'customer': 2, 'total': 9, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS29067 is {'provider': 11, 'peer': 4, 'customer': 2, 'total': 17, 'transit': 4, 'sibling': 0}.\n",
      "Degree of AS41369 is {'provider': 4, 'peer': 1, 'customer': 1, 'total': 6, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS41477 is {'provider': 6, 'peer': 1, 'customer': 1, 'total': 8, 'transit': 3, 'sibling': 1}.\n",
      "Degree of AS45634 is {'provider': 2, 'peer': 0, 'customer': 1, 'total': 3, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS211526 is {'provider': 2, 'peer': 3, 'customer': 8, 'total': 13, 'transit': 12, 'sibling': 0}.\n",
      "Degree of AS131659 is {'provider': 8, 'peer': 4, 'customer': 4, 'total': 16, 'transit': 6, 'sibling': 2}.\n",
      "Degree of AS33392 is {'provider': 4, 'peer': 0, 'customer': 3, 'total': 7, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS55944 is {'provider': 6, 'peer': 27, 'customer': 1, 'total': 34, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS60068 is {'provider': 23, 'peer': 514, 'customer': 166, 'total': 703, 'transit': 696, 'sibling': 1}.\n",
      "Degree of AS133800 is {'provider': 5, 'peer': 2, 'customer': 3, 'total': 10, 'transit': 5, 'sibling': 1}.\n",
      "Degree of AS47828 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS6908 is {'provider': 5, 'peer': 73, 'customer': 17, 'total': 95, 'transit': 45, 'sibling': 3}.\n",
      "Degree of AS37680 is {'provider': 6, 'peer': 555, 'customer': 6, 'total': 567, 'transit': 106, 'sibling': 0}.\n",
      "Degree of AS4927 is {'provider': 4, 'peer': 27, 'customer': 3, 'total': 34, 'transit': 25, 'sibling': 0}.\n",
      "Degree of AS62371 is {'provider': 3, 'peer': 38, 'customer': 1, 'total': 42, 'transit': 41, 'sibling': 0}.\n",
      "Degree of AS15301 is {'provider': 5, 'peer': 2, 'customer': 1, 'total': 8, 'transit': 5, 'sibling': 1}.\n",
      "Degree of AS136209 is {'provider': 6, 'peer': 0, 'customer': 1, 'total': 7, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS48173 is {'provider': 6, 'peer': 101, 'customer': 6, 'total': 113, 'transit': 109, 'sibling': 0}.\n",
      "Degree of AS205892 is {'provider': 5, 'peer': 83, 'customer': 1, 'total': 89, 'transit': 88, 'sibling': 0}.\n",
      "Degree of AS132167 is {'provider': 4, 'peer': 30, 'customer': 13, 'total': 47, 'transit': 24, 'sibling': 0}.\n",
      "Degree of AS40068 is {'provider': 3, 'peer': 1, 'customer': 1, 'total': 5, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS19637 is {'provider': 25, 'peer': 3, 'customer': 1, 'total': 29, 'transit': 2, 'sibling': 1}.\n",
      "Degree of AS52423 is {'provider': 4, 'peer': 2, 'customer': 1, 'total': 7, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS21723 is {'provider': 8, 'peer': 12, 'customer': 6, 'total': 26, 'transit': 25, 'sibling': 1}.\n",
      "Degree of AS40160 is {'provider': 6, 'peer': 6, 'customer': 4, 'total': 16, 'transit': 12, 'sibling': 2}.\n",
      "Degree of AS15600 is {'provider': 5, 'peer': 50, 'customer': 2, 'total': 57, 'transit': 45, 'sibling': 0}.\n",
      "Degree of AS25843 is {'provider': 7, 'peer': 2, 'customer': 2, 'total': 11, 'transit': 7, 'sibling': 0}.\n",
      "Degree of AS36086 is {'provider': 7, 'peer': 35, 'customer': 48, 'total': 90, 'transit': 88, 'sibling': 1}.\n",
      "Degree of AS50446 is {'provider': 4, 'peer': 44, 'customer': 2, 'total': 50, 'transit': 6, 'sibling': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of AS136462 is {'provider': 6, 'peer': 0, 'customer': 1, 'total': 7, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS5408 is {'provider': 3, 'peer': 7, 'customer': 30, 'total': 40, 'transit': 38, 'sibling': 0}.\n",
      "Degree of AS54574 is {'provider': 1, 'peer': 4, 'customer': 1, 'total': 6, 'transit': 4, 'sibling': 1}.\n",
      "Degree of AS5430 is {'provider': 5, 'peer': 110, 'customer': 2, 'total': 117, 'transit': 114, 'sibling': 0}.\n",
      "Degree of AS7483 is {'provider': 14, 'peer': 8, 'customer': 1, 'total': 23, 'transit': 12, 'sibling': 0}.\n",
      "Degree of AS13649 is {'provider': 5, 'peer': 1, 'customer': 173, 'total': 179, 'transit': 177, 'sibling': 0}.\n",
      "Degree of AS7506 is {'provider': 4, 'peer': 18, 'customer': 4, 'total': 26, 'transit': 23, 'sibling': 3}.\n",
      "Degree of AS216425 is {'provider': 6, 'peer': 0, 'customer': 1, 'total': 7, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS23930 is {'provider': 8, 'peer': 43, 'customer': 6, 'total': 57, 'transit': 49, 'sibling': 1}.\n",
      "Degree of AS1421 is {'provider': 5, 'peer': 9, 'customer': 6, 'total': 20, 'transit': 10, 'sibling': 1}.\n",
      "Degree of AS5539 is {'provider': 9, 'peer': 92, 'customer': 5, 'total': 106, 'transit': 103, 'sibling': 1}.\n",
      "Degree of AS58791 is {'provider': 4, 'peer': 3, 'customer': 3, 'total': 10, 'transit': 9, 'sibling': 3}.\n",
      "Degree of AS28103 is {'provider': 4, 'peer': 0, 'customer': 10, 'total': 14, 'transit': 13, 'sibling': 0}.\n",
      "Degree of AS263625 is {'provider': 3, 'peer': 23, 'customer': 11, 'total': 37, 'transit': 36, 'sibling': 0}.\n",
      "Degree of AS28110 is {'provider': 6, 'peer': 1, 'customer': 1, 'total': 8, 'transit': 4, 'sibling': 0}.\n",
      "Degree of AS15830 is {'provider': 30, 'peer': 412, 'customer': 508, 'total': 950, 'transit': 944, 'sibling': 0}.\n",
      "Degree of AS5715 is {'provider': 5, 'peer': 1, 'customer': 1, 'total': 7, 'transit': 6, 'sibling': 1}.\n",
      "Degree of AS15989 is {'provider': 3, 'peer': 0, 'customer': 2, 'total': 5, 'transit': 4, 'sibling': 0}.\n",
      "Degree of AS42615 is {'provider': 5, 'peer': 5, 'customer': 1, 'total': 11, 'transit': 8, 'sibling': 0}.\n",
      "Degree of AS24203 is {'provider': 5, 'peer': 38, 'customer': 18, 'total': 61, 'transit': 60, 'sibling': 2}.\n",
      "Degree of AS394968 is {'provider': 4, 'peer': 8, 'customer': 9, 'total': 21, 'transit': 20, 'sibling': 0}.\n",
      "Degree of AS399077 is {'provider': 8, 'peer': 44, 'customer': 3, 'total': 55, 'transit': 45, 'sibling': 1}.\n",
      "Degree of AS138995 is {'provider': 12, 'peer': 5, 'customer': 1, 'total': 18, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS139006 is {'provider': 4, 'peer': 0, 'customer': 1, 'total': 5, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS34624 is {'provider': 6, 'peer': 92, 'customer': 1, 'total': 99, 'transit': 3, 'sibling': 1}.\n",
      "Degree of AS12130 is {'provider': 5, 'peer': 1, 'customer': 1, 'total': 7, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS399244 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS12179 is {'provider': 3, 'peer': 0, 'customer': 19, 'total': 22, 'transit': 21, 'sibling': 2}.\n",
      "Degree of AS38843 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS47065 is {'provider': 6, 'peer': 114, 'customer': 1, 'total': 121, 'transit': 96, 'sibling': 0}.\n",
      "Degree of AS4058 is {'provider': 16, 'peer': 50, 'customer': 15, 'total': 81, 'transit': 60, 'sibling': 0}.\n",
      "Degree of AS59371 is {'provider': 7, 'peer': 18, 'customer': 2, 'total': 27, 'transit': 23, 'sibling': 0}.\n",
      "Degree of AS14325 is {'provider': 5, 'peer': 5, 'customer': 15, 'total': 25, 'transit': 23, 'sibling': 0}.\n",
      "Degree of AS20473 is {'provider': 24, 'peer': 262, 'customer': 510, 'total': 796, 'transit': 787, 'sibling': 1}.\n",
      "Degree of AS55309 is {'provider': 8, 'peer': 1, 'customer': 6, 'total': 15, 'transit': 12, 'sibling': 0}.\n",
      "Degree of AS54811 is {'provider': 2, 'peer': 1, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS20521 is {'provider': 4, 'peer': 0, 'customer': 5, 'total': 9, 'transit': 7, 'sibling': 0}.\n",
      "Degree of AS9299 is {'provider': 11, 'peer': 127, 'customer': 179, 'total': 317, 'transit': 231, 'sibling': 9}.\n",
      "Degree of AS147060 is {'provider': 2, 'peer': 0, 'customer': 1, 'total': 3, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS17072 is {'provider': 14, 'peer': 31, 'customer': 37, 'total': 82, 'transit': 80, 'sibling': 2}.\n",
      "Degree of AS9919 is {'provider': 2, 'peer': 0, 'customer': 1, 'total': 3, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS135947 is {'provider': 2, 'peer': 0, 'customer': 1, 'total': 3, 'transit': 2, 'sibling': 1}.\n",
      "Degree of AS17686 is {'provider': 5, 'peer': 21, 'customer': 3, 'total': 29, 'transit': 28, 'sibling': 0}.\n",
      "Degree of AS18206 is {'provider': 2, 'peer': 4, 'customer': 1, 'total': 7, 'transit': 2, 'sibling': 2}.\n",
      "Degree of AS57144 is {'provider': 4, 'peer': 0, 'customer': 1, 'total': 5, 'transit': 5, 'sibling': 1}.\n",
      "Degree of AS9658 is {'provider': 10, 'peer': 144, 'customer': 38, 'total': 192, 'transit': 180, 'sibling': 4}.\n",
      "Degree of AS13768 is {'provider': 7, 'peer': 131, 'customer': 28, 'total': 166, 'transit': 144, 'sibling': 0}.\n",
      "Degree of AS10204 is {'provider': 5, 'peer': 8, 'customer': 5, 'total': 18, 'transit': 17, 'sibling': 0}.\n",
      "Degree of AS11273 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 1}.\n",
      "Degree of AS208428 is {'provider': 2, 'peer': 0, 'customer': 2, 'total': 4, 'transit': 4, 'sibling': 1}.\n",
      "Degree of AS56460 is {'provider': 1, 'peer': 0, 'customer': 1, 'total': 2, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS396955 is {'provider': 4, 'peer': 0, 'customer': 3, 'total': 7, 'transit': 4, 'sibling': 5}.\n",
      "Degree of AS25773 is {'provider': 10, 'peer': 15, 'customer': 1, 'total': 26, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS394453 is {'provider': 5, 'peer': 0, 'customer': 2, 'total': 7, 'transit': 7, 'sibling': 1}.\n",
      "Degree of AS45815 is {'provider': 7, 'peer': 2, 'customer': 2, 'total': 11, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS14080 is {'provider': 7, 'peer': 4, 'customer': 22, 'total': 33, 'transit': 32, 'sibling': 2}.\n",
      "Degree of AS23352 is {'provider': 10, 'peer': 27, 'customer': 33, 'total': 70, 'transit': 68, 'sibling': 0}.\n",
      "Degree of AS12625 is {'provider': 4, 'peer': 98, 'customer': 1, 'total': 103, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS140637 is {'provider': 2, 'peer': 20, 'customer': 1, 'total': 23, 'transit': 2, 'sibling': 1}.\n",
      "Degree of AS13767 is {'provider': 6, 'peer': 21, 'customer': 78, 'total': 105, 'transit': 102, 'sibling': 0}.\n",
      "Degree of AS206283 is {'provider': 2, 'peer': 0, 'customer': 3, 'total': 5, 'transit': 5, 'sibling': 3}.\n",
      "Degree of AS33745 is {'provider': 3, 'peer': 0, 'customer': 1, 'total': 4, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS26496 is {'provider': 6, 'peer': 38, 'customer': 7, 'total': 51, 'transit': 36, 'sibling': 7}.\n",
      "Degree of AS29066 is {'provider': 8, 'peer': 1, 'customer': 2, 'total': 11, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS43281 is {'provider': 4, 'peer': 0, 'customer': 2, 'total': 6, 'transit': 3, 'sibling': 0}.\n",
      "Degree of AS398107 is {'provider': 3, 'peer': 1, 'customer': 1, 'total': 5, 'transit': 2, 'sibling': 0}.\n",
      "Degree of AS20773 is {'provider': 6, 'peer': 133, 'customer': 8, 'total': 147, 'transit': 144, 'sibling': 6}.\n",
      "Degree of AS7341 is {'provider': 5, 'peer': 3, 'customer': 3, 'total': 11, 'transit': 8, 'sibling': 0}.\n",
      "Degree of AS45753 is {'provider': 6, 'peer': 3, 'customer': 1, 'total': 10, 'transit': 5, 'sibling': 0}.\n",
      "Degree of AS22356 is {'provider': 5, 'peer': 714, 'customer': 48, 'total': 767, 'transit': 762, 'sibling': 0}.\n",
      "Degree of AS9051 is {'provider': 2, 'peer': 0, 'customer': 17, 'total': 19, 'transit': 18, 'sibling': 0}.\n",
      "Degree of AS37468 is {'provider': 8, 'peer': 2635, 'customer': 157, 'total': 2800, 'transit': 2708, 'sibling': 0}.\n",
      "Degree of AS19305 is {'provider': 5, 'peer': 1, 'customer': 1, 'total': 7, 'transit': 4, 'sibling': 1}.\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "# Check how many of them are stub\n",
    "# After TMA\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/after_tma/customers_ases_scrubber_all_01_dec_2024.csv\")\n",
    "ases = df[\"as\"]\n",
    "\n",
    "transit_counter = 0\n",
    "stub_counter = 0\n",
    "\n",
    "for asn in ases:\n",
    "    # Check customer AS using ASRank\n",
    "    as_rank = AsRank(\"2024-12-01\")\n",
    "    asn_degree = as_rank.get_degree(str(asn))\n",
    "    customer_count = asn_degree[\"customer\"]\n",
    "    if customer_count > 0:\n",
    "        transit_counter += 1\n",
    "        print(f\"Degree of AS{asn} is {asn_degree}.\")\n",
    "print(transit_counter)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a58fa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1730"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa01b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/customers_ases_unique_scrubber_all_2024.csv\")\n",
    "ases = df[\"as\"]\n",
    "len(ases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of total ISPs, check how many of them are stub and transit ASes\n",
    "# Category 1 - Layer 2 is an ISP\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/customers_ases_scrubber_all_2024.csv\")\n",
    "as_list = df[\"as\"].unique()\n",
    "\n",
    "\n",
    "# Load ASdb CSV file into a pandas DataFrame\n",
    "asdb = pd.read_csv(\"/home/shyam/jupy/ddos_scrubber/data/\"+year+\"-01_categorized_ases.csv\", low_memory=False)\n",
    "\n",
    "# Ensure AS numbers in the CSV are formatted correctly as strings and strip extra spaces\n",
    "asdb['ASN'] = asdb['ASN'].astype(str).str.strip()\n",
    "\n",
    "# # Put \"AS\" prefix prepend as the file contains only numbers\n",
    "as_list = [f'AS{asn}' for asn in as_list]\n",
    "\n",
    "# Filter ASNs with category 'ISP' that are in the list\n",
    "isp_ases = asdb[(asdb['Category 1 - Layer 2'] == 'Internet Service Provider (ISP)') & (asdb['ASN'].isin(as_list))]['ASN'].tolist()\n",
    "\n",
    "# Remove the \"AS\" prefix\n",
    "ases = [as_num.replace(\"AS\", \"\") for as_num in isp_ases]\n",
    "\n",
    "# Check how many of them are stub and transit\n",
    "as_rank = AsRank(\"2024-01-01\")\n",
    "# Check how many of them are stub\n",
    "transit_counter = 0\n",
    "for asn in ases:\n",
    "    # Check customer AS using ASRank\n",
    "    as_rank = AsRank(\"2024-01-01\")\n",
    "    asn_degree = as_rank.get_degree(str(asn))\n",
    "    customer_count = asn_degree[\"customer\"]\n",
    "    if customer_count > 0:\n",
    "        transit_counter += 1\n",
    "#         print(f\"Degree of AS{asn} is {asn_degree}.\")\n",
    "print(transit_counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04de4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4]\n",
    "list2 = [3, 4, 5]\n",
    "list3 = [4, 6, 7]\n",
    "\n",
    "all_unique_values = set(list1) | set(list2) | set(list3)  # Union\n",
    "\n",
    "print(\"Values appearing in at least one list:\", all_unique_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
