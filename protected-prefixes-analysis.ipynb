{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the protected prefixes correlated with A records of Tranco 1M list in 2024 jan 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the protected prefixes of individual scrubbers\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "scrubber_asns = [\"32787\", \"13335\", \"19551\", \"198949\", \"19905\"]\n",
    "year = \"2017\"\n",
    "\n",
    "path = \"/home/shyam/jupy/ddos_scrubber/data/\"\n",
    "all_record_prefixes = [] # Stores number of customer prefixes for a month\n",
    "\n",
    "\n",
    "for scrubber_asn in scrubber_asns:\n",
    "    single_record_prefixes = [] # Stores number of customer prefixes for a month\n",
    "\n",
    "    confirmed_customers1 = pd.read_csv(path + \"confirmed_customers_as\"+scrubber_asn+\"_\" +year+\".csv\")\n",
    "    confirmed_customers1_unique_origin_prefixes = confirmed_customers1['prefix'].unique()\n",
    "    \n",
    "    if scrubber_asn not in [\"13335\"]: # Cloudflare does not have path prepending case\n",
    "        path_prepending = pd.read_csv(path + \"unique_optimized_provider_as\"+scrubber_asn+\"_path_prepend_01_jan_\"+year+\".csv\")\n",
    "        path_prepending_unique_origin_prefixes = path_prepending['prefix'].unique()\n",
    "        confirmed_customers2_prefixes = list(set(path_prepending_unique_origin_prefixes)-set(confirmed_customers1_unique_origin_prefixes))\n",
    "    else:\n",
    "        confirmed_customers2_prefixes = []\n",
    "        \n",
    "    if  scrubber_asn not in [\"13335\"]: # Cloudflare does not have path prepending case\n",
    "        df = pd.read_csv(path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_jan_\"+year+\"_v3.csv\")\n",
    "        sibling_path = df.loc[(df['siblings'] == 1) & (df['new_provider_sibling_check'] == int(scrubber_asn))]\n",
    "        sibling_path_unique_origin_prefixes = sibling_path['prefix'].unique()\n",
    "    else:\n",
    "        sibling_path_unique_origin_prefixes = []\n",
    "\n",
    "        \n",
    "    confirmed_customers3_prefixes = list(set(sibling_path_unique_origin_prefixes)-set(confirmed_customers2_prefixes)-set(confirmed_customers1_unique_origin_prefixes))\n",
    "\n",
    "    confirmed_customers_prefixes = list(confirmed_customers1_unique_origin_prefixes) + list(confirmed_customers2_prefixes) + list(confirmed_customers3_prefixes)\n",
    "    \n",
    "    # Save to CSV file\n",
    "    with open(\"../data/customers_prefixes_scrubber_\"+scrubber_asn+\"_\"+year+\".csv\", \"w\", newline=\"\") as file:\n",
    "#     with open(\"../data/customers_prefixes_scrubber_\"+scrubber_asn+\"_2024_cloudflare.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prefix\"])  # Write header\n",
    "        for prefix in confirmed_customers_prefixes:\n",
    "            writer.writerow([prefix])  # Write each name in a new row\n",
    "    print(\"Done for scrubber \", scrubber_asn)\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ca629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total protected prefixes of all scrubbers\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "year = \"2017\"\n",
    "\n",
    "pattern = r\"^customers_prefixes_scrubber_\\d+_\"+year+\"\\.csv$\"\n",
    "directory = \"/home/shyam/jupy/ddos_scrubber/data\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(directory) if re.match(pattern, f)]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append it to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(directory+\"/\"+file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Merge all dataframes into one\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe into a new CSV file\n",
    "merged_df.to_csv(\"../data/customers_prefixes_scrubber_all_\"+year+\".csv\", index=False)  \n",
    "\n",
    "merged_df_unique = merged_df.drop_duplicates()\n",
    "\n",
    "# Save the merged dataframe into a new CSV file\n",
    "merged_df_unique.to_csv(\"../data/customers_prefixes_unique_scrubber_all_\"+year+\".csv\", index=False) \n",
    "\n",
    "print(\"Completed. All the prefixes of %s saved in file customers_prefixes_scrubber_all_%s.csv\"  %(year, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5bc4eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing month: jan\n",
      "âœ… Without removing duplicates: 3154\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jan_2020.csv (3153 unique rows)\n",
      "ðŸ”„ Processing month: feb\n",
      "âœ… Without removing duplicates: 3384\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_feb_2020.csv (3383 unique rows)\n",
      "ðŸ”„ Processing month: mar\n",
      "âœ… Without removing duplicates: 3551\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_mar_2020.csv (3550 unique rows)\n",
      "ðŸ”„ Processing month: apr\n",
      "âœ… Without removing duplicates: 3295\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_apr_2020.csv (3292 unique rows)\n",
      "ðŸ”„ Processing month: may\n",
      "âœ… Without removing duplicates: 3414\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_may_2020.csv (3411 unique rows)\n",
      "ðŸ”„ Processing month: jun\n",
      "âœ… Without removing duplicates: 3506\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jun_2020.csv (3503 unique rows)\n",
      "ðŸ”„ Processing month: jul\n",
      "âœ… Without removing duplicates: 3561\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jul_2020.csv (3559 unique rows)\n",
      "ðŸ”„ Processing month: aug\n",
      "âœ… Without removing duplicates: 3686\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_aug_2020.csv (3682 unique rows)\n",
      "ðŸ”„ Processing month: sep\n",
      "âœ… Without removing duplicates: 3910\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_sep_2020.csv (3903 unique rows)\n",
      "ðŸ”„ Processing month: oct\n",
      "âœ… Without removing duplicates: 4390\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_oct_2020.csv (4384 unique rows)\n",
      "ðŸ”„ Processing month: nov\n",
      "âœ… Without removing duplicates: 4703\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_nov_2020.csv (4698 unique rows)\n",
      "ðŸ”„ Processing month: dec\n",
      "âœ… Without removing duplicates: 5086\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_dec_2020.csv (5082 unique rows)\n",
      "âœ… Done for all months for year .2020\n",
      "ðŸ”„ Processing month: jan\n",
      "âœ… Without removing duplicates: 5306\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jan_2021.csv (5301 unique rows)\n",
      "ðŸ”„ Processing month: feb\n",
      "âœ… Without removing duplicates: 5566\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_feb_2021.csv (5551 unique rows)\n",
      "ðŸ”„ Processing month: mar\n",
      "âœ… Without removing duplicates: 5481\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_mar_2021.csv (5480 unique rows)\n",
      "ðŸ”„ Processing month: apr\n",
      "âœ… Without removing duplicates: 5627\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_apr_2021.csv (5626 unique rows)\n",
      "ðŸ”„ Processing month: may\n",
      "âœ… Without removing duplicates: 5834\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_may_2021.csv (5832 unique rows)\n",
      "ðŸ”„ Processing month: jun\n",
      "âœ… Without removing duplicates: 5872\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jun_2021.csv (5868 unique rows)\n",
      "ðŸ”„ Processing month: jul\n",
      "âœ… Without removing duplicates: 6123\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jul_2021.csv (6118 unique rows)\n",
      "ðŸ”„ Processing month: aug\n",
      "âœ… Without removing duplicates: 6338\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_aug_2021.csv (6336 unique rows)\n",
      "ðŸ”„ Processing month: sep\n",
      "âœ… Without removing duplicates: 6505\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_sep_2021.csv (6504 unique rows)\n",
      "ðŸ”„ Processing month: oct\n",
      "âœ… Without removing duplicates: 6808\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_oct_2021.csv (6805 unique rows)\n",
      "ðŸ”„ Processing month: nov\n",
      "âœ… Without removing duplicates: 7158\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_nov_2021.csv (7152 unique rows)\n",
      "ðŸ”„ Processing month: dec\n",
      "âœ… Without removing duplicates: 7260\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_dec_2021.csv (7256 unique rows)\n",
      "âœ… Done for all months for year .2021\n",
      "ðŸ”„ Processing month: jan\n",
      "âœ… Without removing duplicates: 7716\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jan_2022.csv (7245 unique rows)\n",
      "ðŸ”„ Processing month: feb\n",
      "âœ… Without removing duplicates: 7972\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_feb_2022.csv (7452 unique rows)\n",
      "ðŸ”„ Processing month: mar\n",
      "âœ… Without removing duplicates: 8109\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_mar_2022.csv (7630 unique rows)\n",
      "ðŸ”„ Processing month: apr\n",
      "âœ… Without removing duplicates: 8575\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_apr_2022.csv (8146 unique rows)\n",
      "ðŸ”„ Processing month: may\n",
      "âœ… Without removing duplicates: 8945\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_may_2022.csv (8510 unique rows)\n",
      "ðŸ”„ Processing month: jun\n",
      "âœ… Without removing duplicates: 8996\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jun_2022.csv (8567 unique rows)\n",
      "ðŸ”„ Processing month: jul\n",
      "âœ… Without removing duplicates: 9445\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jul_2022.csv (9013 unique rows)\n",
      "ðŸ”„ Processing month: aug\n",
      "âœ… Without removing duplicates: 9489\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_aug_2022.csv (9030 unique rows)\n",
      "ðŸ”„ Processing month: sep\n",
      "âœ… Without removing duplicates: 9163\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_sep_2022.csv (8682 unique rows)\n",
      "ðŸ”„ Processing month: oct\n",
      "âœ… Without removing duplicates: 9524\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_oct_2022.csv (9029 unique rows)\n",
      "ðŸ”„ Processing month: nov\n",
      "âœ… Without removing duplicates: 9791\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_nov_2022.csv (9244 unique rows)\n",
      "ðŸ”„ Processing month: dec\n",
      "âœ… Without removing duplicates: 9914\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_dec_2022.csv (9385 unique rows)\n",
      "âœ… Done for all months for year .2022\n",
      "ðŸ”„ Processing month: jan\n",
      "âœ… Without removing duplicates: 9363\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jan_2023.csv (9353 unique rows)\n",
      "ðŸ”„ Processing month: feb\n",
      "âœ… Without removing duplicates: 9353\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_feb_2023.csv (9344 unique rows)\n",
      "ðŸ”„ Processing month: mar\n",
      "âœ… Without removing duplicates: 9430\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_mar_2023.csv (9422 unique rows)\n",
      "ðŸ”„ Processing month: apr\n",
      "âœ… Without removing duplicates: 9511\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_apr_2023.csv (9502 unique rows)\n",
      "ðŸ”„ Processing month: may\n",
      "âœ… Without removing duplicates: 9804\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_may_2023.csv (9796 unique rows)\n",
      "ðŸ”„ Processing month: jun\n",
      "âœ… Without removing duplicates: 9918\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jun_2023.csv (9910 unique rows)\n",
      "ðŸ”„ Processing month: jul\n",
      "âœ… Without removing duplicates: 10144\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jul_2023.csv (10136 unique rows)\n",
      "ðŸ”„ Processing month: aug\n",
      "âœ… Without removing duplicates: 10238\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_aug_2023.csv (10230 unique rows)\n",
      "ðŸ”„ Processing month: sep\n",
      "âœ… Without removing duplicates: 10374\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_sep_2023.csv (10365 unique rows)\n",
      "ðŸ”„ Processing month: oct\n",
      "âœ… Without removing duplicates: 10775\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_oct_2023.csv (10760 unique rows)\n",
      "ðŸ”„ Processing month: nov\n",
      "âœ… Without removing duplicates: 10780\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_nov_2023.csv (10769 unique rows)\n",
      "ðŸ”„ Processing month: dec\n",
      "âœ… Without removing duplicates: 10672\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_dec_2023.csv (10662 unique rows)\n",
      "âœ… Done for all months for year .2023\n",
      "ðŸ”„ Processing month: jan\n",
      "âœ… Without removing duplicates: 11961\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jan_2024.csv (11532 unique rows)\n",
      "ðŸ”„ Processing month: feb\n",
      "âœ… Without removing duplicates: 11255\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_feb_2024.csv (10849 unique rows)\n",
      "ðŸ”„ Processing month: mar\n",
      "âœ… Without removing duplicates: 11379\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_mar_2024.csv (10968 unique rows)\n",
      "ðŸ”„ Processing month: apr\n",
      "âœ… Without removing duplicates: 11340\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_apr_2024.csv (10942 unique rows)\n",
      "ðŸ”„ Processing month: may\n",
      "âœ… Without removing duplicates: 11563\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_may_2024.csv (11160 unique rows)\n",
      "ðŸ”„ Processing month: jun\n",
      "âœ… Without removing duplicates: 11902\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jun_2024.csv (11511 unique rows)\n",
      "ðŸ”„ Processing month: jul\n",
      "âœ… Without removing duplicates: 12104\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_jul_2024.csv (11708 unique rows)\n",
      "ðŸ”„ Processing month: aug\n",
      "âœ… Without removing duplicates: 12196\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_aug_2024.csv (11796 unique rows)\n",
      "ðŸ”„ Processing month: sep\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Without removing duplicates: 12514\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_sep_2024.csv (12106 unique rows)\n",
      "ðŸ”„ Processing month: oct\n",
      "âœ… Without removing duplicates: 12740\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_oct_2024.csv (12337 unique rows)\n",
      "ðŸ”„ Processing month: nov\n",
      "âœ… Without removing duplicates: 12845\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_nov_2024.csv (12448 unique rows)\n",
      "ðŸ”„ Processing month: dec\n",
      "âœ… Without removing duplicates: 12713\n",
      "ðŸ“ Saved merged file: ../data/after_tma/customers_prefixes_scrubber_all_01_dec_2024.csv (12362 unique rows)\n",
      "âœ… Done for all months for year .2024\n"
     ]
    }
   ],
   "source": [
    "# Find total protected prefixes of all scrubbers monthwise\n",
    "# After TMA\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "years = [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
    "day = \"01\"\n",
    "months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "# List your scrubber ASNs here\n",
    "scrubber_asns = [\"32787\", \"13335\", \"19551\", \"198949\", \"19905\"]\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    for mon in months:\n",
    "        print(f\"ðŸ”„ Processing month: {mon}\")\n",
    "\n",
    "        all_matching_files = []\n",
    "\n",
    "        for scrubber in scrubber_asns:\n",
    "            base_dir = f\"../data/after_tma/as{scrubber}\"\n",
    "\n",
    "            # Regex to match files like customers_prefixes_scrubber_13335_01_jan_2024.csv\n",
    "            pattern = rf\"^customers_prefixes_scrubber_\\d+_{day}_{mon}_{year}\\.csv$\"\n",
    "\n",
    "            for root, dirs, files in os.walk(base_dir):\n",
    "                for file in files:\n",
    "                    if re.match(pattern, file):\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        all_matching_files.append(full_path)\n",
    "\n",
    "        # Merge all matching files\n",
    "        if all_matching_files:\n",
    "            dataframes = [pd.read_csv(f) for f in all_matching_files]\n",
    "            merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "            print(f\"âœ… Without removing duplicates: {len(merged_df)}\")\n",
    "            # Optional: Drop duplicates\n",
    "            merged_df_unique = merged_df.drop_duplicates()\n",
    "\n",
    "            # Output path\n",
    "            output_file = f\"../data/after_tma/customers_prefixes_scrubber_all_{day}_{mon}_{year}.csv\"\n",
    "            merged_df_unique.to_csv(output_file, index=False)\n",
    "            print(f\"ðŸ“ Saved merged file: {output_file} ({len(merged_df_unique)} unique rows)\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ No files found for {mon} {year}\")\n",
    "\n",
    "    print(f\"âœ… Done for all months for year .{year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95855d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of IP addresses: 10880332376531662573245952\n"
     ]
    }
   ],
   "source": [
    "# Find the number of IP addreses in the protected prefix list\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "year = \"2017\"\n",
    "\n",
    "def count_ips_in_prefix(prefix):\n",
    "    \"\"\"Calculate the total number of IP addresses in a given prefix.\"\"\"\n",
    "    return 2 ** (ipaddress.ip_network(prefix, strict=False).max_prefixlen - ipaddress.ip_network(prefix).prefixlen)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"../data/customers_prefixes_unique_scrubber_all_\"+year+\".csv\"  # Update this with your actual file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Assuming the column containing prefixes is named 'prefix'\n",
    "total_ips = df['prefix'].apply(count_ips_in_prefix).sum()\n",
    "\n",
    "print(f\"Total number of IP addresses: {total_ips}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f839fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Independent prefixes: 10267\n"
     ]
    }
   ],
   "source": [
    "# Check how many prefixes contain super prefix\n",
    "from ipaddress import ip_network\n",
    "import pandas as pd\n",
    "year = \"2024\"\n",
    "\n",
    "def process_networks(networks):\n",
    "    superprefixes = set()\n",
    "    independent_prefixes = set(networks)\n",
    "\n",
    "    for i, net in enumerate(networks):\n",
    "        for j in range(i):  # Only check prefixes before the current one (less specific ones)\n",
    "            if networks[j].supernet_of(net):\n",
    "                superprefixes.add(networks[j])\n",
    "                independent_prefixes.discard(net)\n",
    "\n",
    "    return superprefixes, independent_prefixes\n",
    "\n",
    "def find_superprefixes_and_independent(prefix_list):\n",
    "    # Convert strings to ip_network objects\n",
    "    networks = [ip_network(p) for p in prefix_list]\n",
    "    \n",
    "    # Separate IPv4 and IPv6 networks\n",
    "    ipv4_networks = sorted([n for n in networks if n.version == 4], key=lambda net: net.prefixlen)\n",
    "    ipv6_networks = sorted([n for n in networks if n.version == 6], key=lambda net: net.prefixlen)\n",
    "    \n",
    "    superprefixes_v4, independent_v4 = process_networks(ipv4_networks)\n",
    "    superprefixes_v6, independent_v6 = process_networks(ipv6_networks)\n",
    "    \n",
    "    return superprefixes_v4 | superprefixes_v6, independent_v4 | independent_v6 # Union merge of the results\n",
    "\n",
    "# Example usage\n",
    "# prefixes = [\n",
    "#     \"192.168.0.0/16\",\n",
    "#     \"192.168.1.0/24\",\n",
    "#     \"2001:db8::/32\",\n",
    "#     \"2001:db8:1::/48\",\n",
    "#     \"10.0.0.0/8\",\n",
    "#     \"10.1.0.0/16\",\n",
    "#     \"202.70.64.0/24\",\n",
    "#     \"2001:4860::/32\",\n",
    "#     \"2001:4860:4860::8888/128\"\n",
    "# ]\n",
    "\n",
    "df = pd.read_csv(\"../data/customers_prefixes_unique_scrubber_all_\"+year+\".csv\")\n",
    "prefixes = df[\"prefix\"]\n",
    "\n",
    "superprefixes, independent_prefixes = find_superprefixes_and_independent(prefixes)\n",
    "# print(\"Superprefixes:\", superprefixes)\n",
    "# print(\"Independent prefixes:\", independent_prefixes)\n",
    "print(\"Count of Independent prefixes:\", len(independent_prefixes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc386299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the ip addresses with the protected prefixes using pytricia loop\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "import pytricia\n",
    "\n",
    "\n",
    "def build_prefix_trees(prefix_list):\n",
    "    \"\"\"Builds two Patricia Tries: one for IPv4 and one for IPv6.\"\"\"\n",
    "    pt_v4 = pytricia.PyTricia()\n",
    "    pt_v6 = pytricia.PyTricia()\n",
    "\n",
    "    for prefix in prefix_list:\n",
    "        network = ipaddress.ip_network(prefix)\n",
    "        if network.version == 4:\n",
    "\n",
    "            pt_v4[prefix] = True\n",
    "        else:\n",
    "            pt_v6[prefix] = True\n",
    "\n",
    "    return pt_v4, pt_v6\n",
    "\n",
    "def count_covered_ips(ip_list, pt_v4, pt_v6):\n",
    "    \"\"\"Counts how many IPs are covered by the prefixes in the Patricia Tries.\"\"\"\n",
    "    count = 0\n",
    "    for ip in ip_list:\n",
    "        ip_obj = ipaddress.ip_address(ip)\n",
    "        if ip_obj.version == 4 and pt_v4.get(ip):\n",
    "            count += 1\n",
    "        elif ip_obj.version == 6 and pt_v6.get(ip):\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_covered_ips(ip_list, pt_v4, pt_v6):\n",
    "    \"\"\"Returns the list of IPs covered by prefixes in the Patricia Trie.\"\"\"\n",
    "    covered_ips = []\n",
    "    for ip in ip_list:\n",
    "        ip_obj = ipaddress.ip_address(ip)\n",
    "        if ip_obj.version == 4 and pt_v4.get(ip):\n",
    "            covered_ips.append(ip)\n",
    "        elif ip_obj.version == 6 and pt_v6.get(ip):\n",
    "            covered_ips.append(ip)\n",
    "#     covered_ips = [ip for ip in ip_list if prefix_tree.get(ip)]\n",
    "    return covered_ips\n",
    "\n",
    "\n",
    "\n",
    "# Protected prefixes lists using all five scrubberscovered_count\n",
    "df = pd.read_csv(\"../data/customers_prefixes_scrubber_all_\"+year+\".csv\") \n",
    "protected_prefixes = df[\"prefix\"].tolist()\n",
    "\n",
    "# Get A record of tranco list of 1M domains\n",
    "# tranco_domain_ip_list = []\n",
    "\n",
    "tranco_ip_addresses = []\n",
    "\n",
    "# Get matching domain lists\n",
    "# tranco_domains = []\n",
    "\n",
    "with open(\"../data/tranco-1m/a_records_tranco.txt\", \"r\") as infile:\n",
    "    for line in infile:\n",
    "        tranco_dict = {}\n",
    "        \n",
    "        if \" A \" in line:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                ip = parts[2]\n",
    "                \n",
    "                tranco_ip_addresses.append(parts[2])\n",
    "                domain = parts[0].rstrip('.')\n",
    "                tranco_dict[ip] = domain\n",
    "#                 tranco_domains.append(domain)\n",
    "#     tranco_domain_ip_list.append(tranco_dict)\n",
    "    \n",
    "\n",
    "# Build separate prefix trees for IPv4 and IPv6\n",
    "pt_v4, pt_v6 = build_prefix_trees(protected_prefixes)\n",
    "\n",
    "# Find the number of IPs covered\n",
    "covered_count = count_covered_ips(tranco_ip_addresses, pt_v4, pt_v6)\n",
    "\n",
    "print(f\"Number of IPs covered: {covered_count}\")\n",
    "\n",
    "# Find the IPs covered\n",
    "covered_ips = get_covered_ips(tranco_ip_addresses, pt_v4, pt_v6)\n",
    "\n",
    "unique_covered_ips = list(set(covered_ips))\n",
    "print(f\"Number of unique IPs covered: {len(unique_covered_ips)}\")\n",
    "\n",
    "# Save covered ips in a .txt file.\n",
    "with open(\"../data/tranco-1m/tranco_scrubber_covered_ip.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ip in unique_covered_ips:\n",
    "        f.write(f\"{ip}\\n\")\n",
    "print(\"Completed.\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED FROM HERE, We used OpenIntel instead\n",
    "# Find ranks of tranco 1M domains with their A records (IP address/es) and save it into a file.\n",
    "import csv\n",
    "\n",
    "# Load domains and assign ranks\n",
    "rank_dict = {}\n",
    "with open(\"../data/tranco-1m/top-1m.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for rank, domain in enumerate(f, start=1):  # Auto-assign rank\n",
    "        domain = domain.strip()\n",
    "        if domain:\n",
    "            rank_dict[domain] = str(rank)\n",
    "\n",
    "# Parse massdns results\n",
    "resolved = {}\n",
    "with open(\"../data/tranco-1m/a_records_tranco.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 3 and parts[1] == \"A\":\n",
    "            domain = parts[0].rstrip('.')\n",
    "            ip = parts[2]\n",
    "            if domain not in resolved:\n",
    "                resolved[domain] = []\n",
    "            resolved[domain].append(ip) # Same domain can  have multiple ip addresses\n",
    "\n",
    "# Write final results with rank\n",
    "with open(\"../data/tranco-1m/tranco_resolved_with_rank.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Rank\", \"Domain\", \"IPs\"])\n",
    "    for domain, ips in resolved.items():\n",
    "        writer.writerow([rank_dict.get(domain, \"N/A\"), domain, \",\".join(ips)])\n",
    "\n",
    "print(\"Resolution completed: Check tranco_resolved_with_rank.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rank, domain names of the covered ip addresses.\n",
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# List of target IP addresses\n",
    "tranco_scrubber_covered_ips = []\n",
    "\n",
    "with open(\"../data/tranco-1m/tranco_scrubber_covered_ip.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        tranco_scrubber_covered_ips.append(line.strip())\n",
    "\n",
    "\n",
    "target_ips = set(tranco_scrubber_covered_ips)\n",
    "\n",
    "# Sample CSV data (as a string for demonstration)\n",
    "\n",
    "# csv_data = \"\"\"Rank,Domain,IPs\n",
    "# 1,google.com,142.250.74.78,172.217.14.206\n",
    "# 2,youtube.com,172.217.14.206\n",
    "# 3,example.com,8.8.8.8\n",
    "# 4,another-example.com,8.8.8.8\n",
    "# \"\"\"\n",
    "\n",
    "# Load the CSV into a dictionary {IP -> [(Rank, Domain), (Rank, Domain), ...]}\n",
    "ip_to_rank = {}\n",
    "\n",
    "# Read the CSV object\n",
    "csv_file = io.StringIO(\"../data/tranco-1m/tranco_resolved_with_rank.csv\")\n",
    "reader = csv.reader(csv_file)\n",
    "\n",
    "\n",
    "with open('../data/tranco-1m/tranco_resolved_with_rank.csv', mode ='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    next(csvFile)  # Skip header\n",
    "    \n",
    "    for row in csvFile:\n",
    "        rank, domain, *ips = row  # Extract rank, domain, and IPs\n",
    "        for ip in ips:\n",
    "            ip = ip.strip()\n",
    "            if ip:\n",
    "                if ip not in ip_to_rank:\n",
    "                    ip_to_rank[ip] = []\n",
    "                ip_to_rank[ip].append((rank, domain))  # Store multiple (Rank, Domain) pairs\n",
    "\n",
    "\n",
    "\n",
    "# Find matching domains and ranks\n",
    "results = []\n",
    "for ip in target_ips:\n",
    "    if ip in ip_to_rank:\n",
    "        ranks, domains = zip(*ip_to_rank[ip])  # Unpack into separate lists\n",
    "        results.append((ip, \";\".join(ranks), \";\".join(domains)))  # Store all matches\n",
    "    else:\n",
    "        results.append((ip, \"N/A\", \"N/A\"))  # No match found\n",
    "\n",
    "# Save results to a CSV file\n",
    "with open(\"../data/tranco-1m/ip_domains_ranks.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"IP\", \"Rank(s)\", \"Domain(s)\"])  # Write header\n",
    "    \n",
    "    # Write the results rows\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Results saved to 'ip_domains_ranks.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ranks of the domains\n",
    "import csv\n",
    "\n",
    "# List to store ranks\n",
    "ranks = []\n",
    "\n",
    "# Read the CSV file to extract ranks\n",
    "with open(\"../data/tranco-1m/ip_domains_ranks.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        # row[1] contains Rank(s), which is a semicolon-separated string\n",
    "        if row[1] != \"N/A\":\n",
    "            ranks.extend(row[1].split(\";\"))  # Add each rank to the list\n",
    "\n",
    "# Convert ranks to integers (if needed)\n",
    "ranks = [int(rank) for rank in ranks]\n",
    "\n",
    "# Print the ranks list\n",
    "print(\"No. of domains covered by 7985 IP addresses are %s\" %len(ranks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95403c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of domains hosted by those IP addresses\n",
    "# Reverse IP lookup, results saved in /home/shyam/jupy/ddos_scrubber/data/tranco-1m/ttranco_scrubber_covered_ptr_results.txt\n",
    "\n",
    "from ipaddress import ip_address\n",
    "\n",
    "# Input and output file names\n",
    "ip_file = \"../data/tranco-1m/tranco_scrubber_covered_ip.txt\"\n",
    "ptr_file = \"../data/tranco-1m/tranco_scrubber_covered_ptr.txt\"\n",
    "\n",
    "# Step 1: Convert IPs to PTR queries\n",
    "def ip_to_ptr(ip):\n",
    "    \"\"\"Convert an IP address to a PTR record format.\"\"\"\n",
    "    reversed_ip = ip_address(ip).reverse_pointer\n",
    "    return reversed_ip\n",
    "    \n",
    "# Step 2: Write converted PTR into a file\n",
    "with open(ip_file, \"r\") as f, open(ptr_file, \"w\") as out:\n",
    "    for line in f:\n",
    "        ip = line.strip()\n",
    "        if ip:\n",
    "            out.write(ip_to_ptr(ip) + \"\\n\")\n",
    "\n",
    "# Run massdns\n",
    "# massdns -r lists/resolvers.txt -t PTR -o S -w /home/shyam/jupy/ddos_scrubber/data/tranco-1m/tranco_scrubber_covered_ptr_results.txt /home/shyam/jupy/ddos_scrubber/data/tranco-1m/tranco_scrubber_covered_ptr.txt\n",
    "# 12769 domains found out of 7985 IP addresses protected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranco 1M list of 01 Jan 2024 showed 14449 domains, while reverse IP lookup on 11 Feb 2025 showed 12769 domains.\n",
    "# Compare these two domain names to find number of same domains and different domains\n",
    "import csv\n",
    "\n",
    "# List to store domains from Tranco 1M list\n",
    "domains_tranco = []\n",
    "\n",
    "# Read the CSV file to extract ranks\n",
    "with open(\"../data/tranco-1m/ip_domains_ranks.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        # row[2] contains Domain(s), which is a semicolon-separated string\n",
    "        if row[2] != \"N/A\":\n",
    "            domains_tranco.extend(row[2].rsplit(\";\"))  # Add each domain to the list\n",
    "\n",
    "# Convert ranks to integers (if needed)\n",
    "domains_tranco = [domain for domain in domains_tranco]\n",
    "\n",
    "# Get domains from reverse IP lookup of 79885 unique IP addrsses on 11 Feb 2025\n",
    "domains_reverse_lookup = []\n",
    "with open(\"../data/tranco-1m/tranco_scrubber_covered_ptr_results.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "#         if len(parts) >= 3 and parts[1] == \"PTR\":\n",
    "        domain = parts[2].rstrip('.')\n",
    "        domains_reverse_lookup.append(domain) \n",
    "\n",
    "# Find unique domains\n",
    "unique_domains_reverse_lookup = list(set(domains_reverse_lookup))\n",
    "unique_domains_tranco = list(set(domains_tranco))\n",
    "\n",
    "print(\"No. of domains in Tranco 1M list covered by 7985 IP addresses are %s and unique number of domains are %s.\\n\" %(len(domains_tranco), len(unique_domains_tranco)) )\n",
    "\n",
    "print(\"No. of domains in reverse IP lookup covered by 7985 IP addresses are %s and unique number of domains are %s.\\n\" %(len(domains_reverse_lookup),(len(unique_domains_reverse_lookup))) )\n",
    "\n",
    "# Similar domains in Tranco 1M list and reverse domain list\n",
    "same_domains = list(set(domains_tranco) & set(unique_domains_reverse_lookup))\n",
    "print(\"No. of same domains in Tranco 1M list and reverse domain list is %s.\\n\" %len(same_domains))\n",
    "\n",
    "domains_tranco_not_reverse = list(set(domains_tranco) - set(domains_reverse_lookup))\n",
    "print(\"No. of domains in Tranco 1M list but not in reverse domain list is %s.\\n\" %len(domains_tranco_not_reverse))\n",
    "\n",
    "domains_reverse_not_tranco = list(set(domains_reverse_lookup) - set(domains_tranco))\n",
    "print(\"No. of domains in reverse domain list but not in Tranco 1M list is %s.\\n\" %len(domains_reverse_not_tranco))\n",
    "\n",
    "total_protected_domains = len(same_domains) + len(domains_tranco_not_reverse) + len(domains_reverse_not_tranco)\n",
    "print(\"Total number of protected domains by scrubbing is %s\" %(total_protected_domains))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
