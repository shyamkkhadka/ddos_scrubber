{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc74111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw bgp data from Aruba machine locally.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/shyam/jupy/ddos_scrubber/data/as32787/2022/’: File exists\n",
      "scp: /home/shyam/data/containers/storage/volumes/shared_dir/_data/2022/raw*: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/home/shyam/data/containers/storage/volumes/shared_dir/_data/2022': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "year = \"2022\" \n",
    "scrubber_asn = \"32787\" # Scrubber asn\n",
    "path = \"/home/shyam/jupy/ddos_scrubber/data/as\"+scrubber_asn+\"/\"+year+\"/\"\n",
    "print(\"Downloading raw bgp data from Aruba machine locally.\\n\")\n",
    "os.system(\"mkdir \"+path)\n",
    "os.system(\"scp aruba-shyam:/home/shyam/data/containers/storage/volumes/shared_dir/_data/\"+year+\"/raw* \"+ path)\n",
    "# os.system(\"scp aruba-shyam:/home/shyam/data/containers/storage/volumes/shared_dir/_data/raw* \"+ path)\n",
    "\n",
    "# Remove old files from Aruba\n",
    "os.system(\"ssh aruba-shyam rm -r /home/shyam/data/containers/storage/volumes/shared_dir/_data/\"+year)\n",
    "print(\"Completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative is to download using pybgpstream\n",
    "import pybgpstream\n",
    "stream = pybgpstream.BGPStream(\n",
    "        from_time=\"2021-01-01 00:00:00 UTC\",\n",
    "        until_time=\"2021-01-01 00:00:00 UTC\",\n",
    "        collectors=[\"route-views.gixa\", \"route-views.kixp\",  \"route-views.napafrica\"],\n",
    "         collectors=[ \"route-views.bdix\",  \"route-views.bknix\",  \"route-views.decix.jhb\",  \"decix.jhb\",  \"decix.jhb\",  \"route-views.wide\",  \"route-views.sg\",  \"kinx.icn\",  \"route-views.phoix\",  \"interlan.otp\",  \"route-views.linx\",  \"route-views.amsix\",  \"namex.fco\",  \"route-views.soxrs\",  \"iraq-ixp.bgw\",  \"route-views.uaeix\",  \"cix.atl\",  \"route-views.ny\",  \"route-views.eqix\",  \"route-views.chicago\",  \"route-views.mwix\",  \"route-views.flix\",  \"route-views.nwax\",  \"pacwave.lax\",  \"route-views.isc\",  \"route-views.sfmix\",  \"route-views.telxatl\",  \"route-views.gorex\",  \"route-views.sydney\",  \"route-views.gorex\",  \"route-views.gorex\",  \"route-views.perth\",  \"route-views.perth\",  \"route-views.perth\",  \"route-views.perth\",  \"route-views.perth\",  \"route-views.fortaleza\",  \"route-views.rio\",  \"route-views2.saopaulo\",  \"route-views.chile\",  \"pit.scl\",  \"pitmx.qro\",  \"route-views.peru\",  \"route-views (Cisco)\",  \"route-views2\",  \"route-views3\",  \"route-views4\",  \"route-views5\",  \"route-views6\",  \"route-views7\",  \"route-views8\"],\n",
    "        record_type=\"ribs\",     \n",
    "        filter = 'path _13335_' #Look for all the prefixes originated by AS15916\n",
    "       )\n",
    "stream.set_data_interface_option(\"broker\", \"cache-dir\", \"/home/shyam/jupy/cache\")\n",
    "    \n",
    "# Find paths from MS ASN to route collectors\n",
    "for rec in stream.records():\n",
    "    for elem in rec:\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9eaa610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Saving raw bgpreader data in .txt format to csv file.\n",
      "\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_jan_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_feb_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_aug_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_may_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_sep_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_dec_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_apr_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_jun_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_jul_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_oct_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_mar_2022.csv\n",
      "Data successfully saved to /home/shyam/jupy/ddos_scrubber/data/as32787/2022/optimized_raw_as32787_01_nov_2022.csv\n",
      "Step 1 completed.\n"
     ]
    }
   ],
   "source": [
    "# Program to find origin, provider, prefix, its length and version from raw data generated using bgpreader as \n",
    "# bgpreader -t ribs -w 1485907200,1485907200 -A _32787_ >> /home/shyam/jupy/ddos_scrubber/data/raw_as32787_01_feb_2017.txt\n",
    "# Use in aruba\n",
    "# Use python multicore programming feature\n",
    "import pandas as pd\n",
    "import csv\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def process_chunk(lines):\n",
    "    \"\"\"Process a chunk of lines and extract prefix, AS path, and origin AS.\"\"\"\n",
    "    chunk_data = []\n",
    "    for line in lines:\n",
    "        fields = line.strip().split('|')\n",
    "        if len(fields) > 12 and fields[1] == \"R\":\n",
    "            try:\n",
    "               # Extract prefix, AS path, and origin AS\n",
    "                prefix = fields[9]\n",
    "                as_path = fields[11].split()\n",
    "\n",
    "\n",
    "                # Find provider here checking AS repetetitions and the same organization owning multiple ASes\n",
    "#                 provider = find_immediate_provider(as_path)\n",
    "                \n",
    "                provider = as_path[-2] if len(as_path) > 1 else None  # The ASN before the origin AS\n",
    "                origin_as = as_path[-1] if as_path else None\n",
    "                \n",
    "                pfx_len = int(prefix.split('/')[1]) if '/' in prefix else None\n",
    "                ip_version = \"IPv6\" if ':' in prefix else \"IPv4\"\n",
    "\n",
    "                # Append data to the chunk\n",
    "                chunk_data.append([prefix, ' '.join(as_path), origin_as, provider, pfx_len, ip_version])\n",
    "            \n",
    "            except IndexError:\n",
    "                continue\n",
    "    return chunk_data\n",
    "\n",
    "def process_route_data_parallel(file_path, output_file, num_workers=8, chunk_size=100000):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = []\n",
    "        \n",
    "        # Initialize CSV output\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['prefix', 'as_path', 'origin_as', 'provider', 'pfx_len', 'ip_version'])\n",
    "\n",
    "        # Process the file in chunks with multiple workers\n",
    "        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = []\n",
    "            for line in file:\n",
    "                lines.append(line)\n",
    "                \n",
    "                # When lines reach chunk size, process them\n",
    "                if len(lines) >= chunk_size:\n",
    "                    futures.append(executor.submit(process_chunk, lines))\n",
    "                    lines = []\n",
    "\n",
    "            # Process any remaining lines after the loop\n",
    "            if lines:\n",
    "                futures.append(executor.submit(process_chunk, lines))\n",
    "\n",
    "            # Collect results from all futures and write to CSV\n",
    "            for future in futures:\n",
    "                chunk_data = future.result()\n",
    "                if chunk_data:\n",
    "                    df = pd.DataFrame(chunk_data, columns=['prefix', 'as_path', 'origin_as', 'provider', 'pfx_len', 'ip_version'])\n",
    "                    df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "\n",
    "pattern = r\"raw_as\"+scrubber_asn+\".*\\.txt$\"  # Match files with format raw_as32787_*.txt\n",
    "\n",
    "print(\"Step 1: Saving raw bgpreader data in .txt format to csv file.\\n\")\n",
    "\n",
    "# Loop through the txt files that contain RIB snapshots of every month  \n",
    "for filename in os.listdir(path):\n",
    "     # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        mon = filename.split('_')[3] # Get month name from file name\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "        file_path = path + filename\n",
    "        output_file = path + 'optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv'\n",
    "        process_route_data_parallel(file_path, output_file)\n",
    "print(\"Step 1 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49933253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Removing duplicates.\n",
      "Step 2 completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: Removing duplicates.\")\n",
    "# Read csv file and remove duplicate rows\n",
    "pattern = r\"unique_optimized_raw_as\"+scrubber_asn+\".*\\.csv$\"  # Match files with format raw_as32787_*.txt\n",
    "       \n",
    "# Loop through the txt files that contain RIB snapshots of every month  \n",
    "for filename in os.listdir(path):\n",
    "     # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        mon = filename.split('_')[4] # Get month name from file name\n",
    "#         print(\"Mon is \", mon)\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "        df = pd.read_csv(path + 'optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv', low_memory=False)\n",
    " \n",
    "        # Remove duplicate rows in dataframe\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        # Write unique rows into a file\n",
    "        df.to_csv(path + 'unique_optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv', index=False)\n",
    "        \n",
    "        # Remove original file that contains duplicate records\n",
    "        os.remove(path + 'optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv')\n",
    "\n",
    "print(\"Step 2 completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 3: Removing private ASNs.\")\n",
    "# Read csv file and remove private ASNs (64512 – 65534)\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "year = \"2024\" \n",
    "scrubber_asn = \"32787\" # Scrubber asn\n",
    "path = \"/data/shared_dir/ddos_scrubber/as\"+scrubber_asn+\"/\"+year+\"/\"\n",
    "\n",
    "# Generate a set of private ASNs\n",
    "private_asns = set(range(64512, 65535 + 1))\n",
    "\n",
    "# Define a function to check if any ASN in the AS path is private\n",
    "def has_private_asn(as_path):\n",
    "    try:\n",
    "        # Convert the AS path to a list of integers\n",
    "        as_numbers = map(int, as_path.split())\n",
    "        return any(asn in private_asns for asn in as_numbers)\n",
    "    except ValueError:\n",
    "        # Handle cases where the AS path is invalid\n",
    "        return False\n",
    "\n",
    "\n",
    "pattern = r\"unique_optimized_raw_as\"+scrubber_asn+\".*\\.csv$\"  # Match files with format raw_as32787_*.txt\n",
    "       \n",
    "# Loop through the txt files that contain RIB snapshots of every month  \n",
    "for filename in os.listdir(path):\n",
    "     # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[3] # Get day from file name\n",
    "        mon = filename.split('_')[4] # Get month name from file name\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "        date = year + \"-\" + mon + \"-\" + day\n",
    "        \n",
    "        df = pd.read_csv(path + 'unique_optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv', low_memory=False)\n",
    " \n",
    "\n",
    "        # Apply the function to filter rows\n",
    "        mask = df['as_path'].apply(has_private_asn)\n",
    "\n",
    "        # Filter out rows with private ASNs\n",
    "        df_without_private_asns = df[~mask]\n",
    "\n",
    "        # Write rows after removing private ASNs into a file\n",
    "        df_without_private_asns.to_csv(path + 'unique_optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv', index = False)        \n",
    "\n",
    "print(\"Step 3 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 4: Removing AS sets.\")\n",
    "# Read csv file and remove AS sets\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "year = \"2024\" \n",
    "scrubber_asn = \"32787\" # Scrubber asn\n",
    "path = \"/data/shared_dir/ddos_scrubber/as\"+scrubber_asn+\"/\"+year+\"/\"\n",
    "\n",
    "pattern = r\"optimized_raw_as\"+scrubber_asn+\".*\\.csv$\"  # Match files with format raw_as32787_*.txt\n",
    "       \n",
    "# Loop through the txt files that contain RIB snapshots of every month  \n",
    "for filename in os.listdir(path):\n",
    "     # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[4] # Get day from file name\n",
    "        mon = filename.split('_')[5] # Get month name from file name\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "        date = year + \"-\" + mon + \"-\" + day\n",
    "        \n",
    "        df = pd.read_csv(path + 'unique_optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv', low_memory=False)\n",
    " \n",
    "        print(\"Records before %s.\" %len(df)) \n",
    "\n",
    "        print(\"Removing AS set in an AS path\")\n",
    "        # Find rows containing '{}' (set origins)\n",
    "        mask = df['as_path'].str.contains(r'\\{.*\\}', na=False)\n",
    "        rows_with_origin_set = df[mask]\n",
    "\n",
    "        # Count rows with set origin\n",
    "        count_set_origin = rows_with_origin_set.shape[0]\n",
    "\n",
    "        # Remove rows with set origin from the DataFrame\n",
    "        df_cleaned = df[~mask]\n",
    "\n",
    "        # Remove rows with set origin from the DataFrame\n",
    "        df_cleaned = df[~mask]\n",
    "\n",
    "        df_cleaned.to_csv(path + 'unique_optimized_raw_as13335_01_jan_2024.csv', index = False)\n",
    "        print(\"Number of rows with set origin:\", count_set_origin)\n",
    "        print(\"DataFrame after removal:\")\n",
    "        print(\"Records after %s.\" %len(df_cleaned)) \n",
    "        print(\"%s number of records were removed that contains AS set in AS paths.\" %(len(df) - len(df_cleaned)))\n",
    "\n",
    "\n",
    "print(\"Step 4 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7891e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Removing records with scrubber as origin.\n",
      "                  prefix               as_path  origin_as  provider  pfx_len  \\\n",
      "1095    2001:500:2f::/48      14907 13335 3557       3557   13335.0       48   \n",
      "1096    2001:500:2f::/48       6762 13335 3557       3557   13335.0       48   \n",
      "1097    2001:500:2f::/48      19151 13335 3557       3557   13335.0       48   \n",
      "1098    2001:500:2f::/48  7575 4826 13335 3557       3557   13335.0       48   \n",
      "1099    2001:500:a8::/48      6762 13335 21556      21556   13335.0       48   \n",
      "...                  ...                   ...        ...       ...      ...   \n",
      "344658    203.29.53.0/24    43578 13335 209242     209242   13335.0       24   \n",
      "344659   209.141.38.0/23     29680 13335 53667      53667   13335.0       23   \n",
      "344660   209.141.38.0/23     43578 13335 53667      53667   13335.0       23   \n",
      "344661   216.127.76.0/24     29680 13335 36351      36351   13335.0       24   \n",
      "344662   216.127.76.0/24     43578 13335 36351      36351   13335.0       24   \n",
      "\n",
      "       ip_version  \n",
      "1095         IPv6  \n",
      "1096         IPv6  \n",
      "1097         IPv6  \n",
      "1098         IPv6  \n",
      "1099         IPv6  \n",
      "...           ...  \n",
      "344658       IPv4  \n",
      "344659       IPv4  \n",
      "344660       IPv4  \n",
      "344661       IPv4  \n",
      "344662       IPv4  \n",
      "\n",
      "[10544 rows x 6 columns]\n",
      "                 prefix                                           as_path  \\\n",
      "34         2.59.98.0/24                                41666 13335 209167   \n",
      "35         2.59.98.0/24                         208594 12735 13335 209167   \n",
      "36         2.59.98.0/24                                13058 13335 209167   \n",
      "37         2.59.98.0/24                                60539 13335 209167   \n",
      "38         2.59.98.0/24  60539 61302 61302 61302 61302 61302 13335 209167   \n",
      "...                 ...                                               ...   \n",
      "1991539  23.29.145.0/24                                 3741 13335 394303   \n",
      "1991540  23.29.145.0/24                        3561 209 3356 13335 394303   \n",
      "1991541  23.29.145.0/24                                34224 13335 394303   \n",
      "1991542  23.29.145.0/24                           20130 6939 13335 394303   \n",
      "1991543  23.29.145.0/24                                11686 13335 394303   \n",
      "\n",
      "         origin_as  provider  pfx_len ip_version  \n",
      "34          209167   13335.0       24       IPv4  \n",
      "35          209167   13335.0       24       IPv4  \n",
      "36          209167   13335.0       24       IPv4  \n",
      "37          209167   13335.0       24       IPv4  \n",
      "38          209167   13335.0       24       IPv4  \n",
      "...            ...       ...      ...        ...  \n",
      "1991539     394303   13335.0       24       IPv4  \n",
      "1991540     394303   13335.0       24       IPv4  \n",
      "1991541     394303   13335.0       24       IPv4  \n",
      "1991542     394303   13335.0       24       IPv4  \n",
      "1991543     394303   13335.0       24       IPv4  \n",
      "\n",
      "[982602 rows x 6 columns]\n",
      "                 prefix                  as_path  origin_as  provider  \\\n",
      "4         2.59.100.0/22        1798 13335 132839     132839   13335.0   \n",
      "5         5.133.75.0/24    6423 1299 13335 14717      14717   13335.0   \n",
      "6         5.133.75.0/24         1798 13335 14717      14717   13335.0   \n",
      "7        5.145.180.0/24   6423 1299 13335 140079     140079   13335.0   \n",
      "8        5.145.180.0/24        1798 13335 140079     140079   13335.0   \n",
      "...                 ...                      ...        ...       ...   \n",
      "1653839  5.226.181.0/24       23673 13335 209242     209242   13335.0   \n",
      "1653840  5.226.181.0/24        8492 13335 209242     209242   13335.0   \n",
      "1653841  5.226.181.0/24       11686 13335 209242     209242   13335.0   \n",
      "1653842  5.226.181.0/24        3303 13335 209242     209242   13335.0   \n",
      "1653843  5.226.181.0/24  20130 6939 13335 209242     209242   13335.0   \n",
      "\n",
      "         pfx_len ip_version  \n",
      "4             22       IPv4  \n",
      "5             24       IPv4  \n",
      "6             24       IPv4  \n",
      "7             24       IPv4  \n",
      "8             24       IPv4  \n",
      "...          ...        ...  \n",
      "1653839       24       IPv4  \n",
      "1653840       24       IPv4  \n",
      "1653841       24       IPv4  \n",
      "1653842       24       IPv4  \n",
      "1653843       24       IPv4  \n",
      "\n",
      "[776494 rows x 6 columns]\n",
      "                  prefix                              as_path  origin_as  \\\n",
      "54      2001:500:2f::/48                    196753 13335 3557       3557   \n",
      "55      2001:500:2f::/48                    202032 13335 3557       3557   \n",
      "56      2001:500:2f::/48                     12637 13335 3557       3557   \n",
      "57      2001:500:2f::/48                     12779 13335 3557       3557   \n",
      "58      2001:500:2f::/48                     15605 13335 3557       3557   \n",
      "...                  ...                                  ...        ...   \n",
      "194776   198.41.248.0/23  35699 2914 13335 13335 13335 132892     132892   \n",
      "194777   198.41.248.0/23  43578 1299 13335 13335 13335 132892     132892   \n",
      "194778   198.41.250.0/24   29680 174 13335 13335 13335 132892     132892   \n",
      "194779   198.41.250.0/24  35699 2914 13335 13335 13335 132892     132892   \n",
      "194780   198.41.250.0/24  43578 1299 13335 13335 13335 132892     132892   \n",
      "\n",
      "        provider  pfx_len ip_version  \n",
      "54       13335.0       48       IPv6  \n",
      "55       13335.0       48       IPv6  \n",
      "56       13335.0       48       IPv6  \n",
      "57       13335.0       48       IPv6  \n",
      "58       13335.0       48       IPv6  \n",
      "...          ...      ...        ...  \n",
      "194776   13335.0       23       IPv4  \n",
      "194777   13335.0       23       IPv4  \n",
      "194778   13335.0       24       IPv4  \n",
      "194779   13335.0       24       IPv4  \n",
      "194780   13335.0       24       IPv4  \n",
      "\n",
      "[2893 rows x 6 columns]\n",
      "                 prefix                                   as_path  origin_as  \\\n",
      "0       103.21.246.0/24                   32709 6939 13335 132892     132892   \n",
      "1       103.21.246.0/24            53828 13335 13335 13335 132892     132892   \n",
      "2       103.21.246.0/24              852 13335 13335 13335 132892     132892   \n",
      "3       103.21.246.0/24  19653 3356 6453 13335 13335 13335 132892     132892   \n",
      "4       103.21.246.0/24  19016 3257 4436 13335 13335 13335 132892     132892   \n",
      "...                 ...                                       ...        ...   \n",
      "35551  162.158.204.0/24              54073 2044 6939 13335 132892     132892   \n",
      "36024    185.122.0.0/24                     1798 174 13335 203898     203898   \n",
      "36025    185.122.0.0/24                6423 209 6453 13335 203898     203898   \n",
      "36026    185.122.0.0/24              54073 2044 6939 13335 203898     203898   \n",
      "36027    185.122.0.0/24                   15301 6939 13335 203898     203898   \n",
      "\n",
      "       provider  pfx_len ip_version  \n",
      "0       13335.0       24       IPv4  \n",
      "1       13335.0       24       IPv4  \n",
      "2       13335.0       24       IPv4  \n",
      "3       13335.0       24       IPv4  \n",
      "4       13335.0       24       IPv4  \n",
      "...         ...      ...        ...  \n",
      "35551   13335.0       24       IPv4  \n",
      "36024   13335.0       24       IPv4  \n",
      "36025   13335.0       24       IPv4  \n",
      "36026   13335.0       24       IPv4  \n",
      "36027   13335.0       24       IPv4  \n",
      "\n",
      "[505 rows x 6 columns]\n",
      "                 prefix                  as_path  origin_as  provider  \\\n",
      "4         2.59.100.0/22        1798 13335 132839     132839   13335.0   \n",
      "5         5.133.75.0/24    6423 1299 13335 14717      14717   13335.0   \n",
      "6         5.133.75.0/24         1798 13335 14717      14717   13335.0   \n",
      "7        5.145.180.0/24   6423 1299 13335 140079     140079   13335.0   \n",
      "8        5.145.180.0/24        1798 13335 140079     140079   13335.0   \n",
      "...                 ...                      ...        ...       ...   \n",
      "1653839  5.226.181.0/24       23673 13335 209242     209242   13335.0   \n",
      "1653840  5.226.181.0/24        8492 13335 209242     209242   13335.0   \n",
      "1653841  5.226.181.0/24       11686 13335 209242     209242   13335.0   \n",
      "1653842  5.226.181.0/24        3303 13335 209242     209242   13335.0   \n",
      "1653843  5.226.181.0/24  20130 6939 13335 209242     209242   13335.0   \n",
      "\n",
      "         pfx_len ip_version  \n",
      "4             22       IPv4  \n",
      "5             24       IPv4  \n",
      "6             24       IPv4  \n",
      "7             24       IPv4  \n",
      "8             24       IPv4  \n",
      "...          ...        ...  \n",
      "1653839       24       IPv4  \n",
      "1653840       24       IPv4  \n",
      "1653841       24       IPv4  \n",
      "1653842       24       IPv4  \n",
      "1653843       24       IPv4  \n",
      "\n",
      "[776494 rows x 6 columns]\n",
      "                  prefix                              as_path  origin_as  \\\n",
      "54      2001:500:2f::/48                    196753 13335 3557       3557   \n",
      "55      2001:500:2f::/48                    202032 13335 3557       3557   \n",
      "56      2001:500:2f::/48                     12637 13335 3557       3557   \n",
      "57      2001:500:2f::/48                     12779 13335 3557       3557   \n",
      "58      2001:500:2f::/48                     15605 13335 3557       3557   \n",
      "...                  ...                                  ...        ...   \n",
      "194776   198.41.248.0/23  35699 2914 13335 13335 13335 132892     132892   \n",
      "194777   198.41.248.0/23  43578 1299 13335 13335 13335 132892     132892   \n",
      "194778   198.41.250.0/24   29680 174 13335 13335 13335 132892     132892   \n",
      "194779   198.41.250.0/24  35699 2914 13335 13335 13335 132892     132892   \n",
      "194780   198.41.250.0/24  43578 1299 13335 13335 13335 132892     132892   \n",
      "\n",
      "        provider  pfx_len ip_version  \n",
      "54       13335.0       48       IPv6  \n",
      "55       13335.0       48       IPv6  \n",
      "56       13335.0       48       IPv6  \n",
      "57       13335.0       48       IPv6  \n",
      "58       13335.0       48       IPv6  \n",
      "...          ...      ...        ...  \n",
      "194776   13335.0       23       IPv4  \n",
      "194777   13335.0       23       IPv4  \n",
      "194778   13335.0       24       IPv4  \n",
      "194779   13335.0       24       IPv4  \n",
      "194780   13335.0       24       IPv4  \n",
      "\n",
      "[2893 rows x 6 columns]\n",
      "                  prefix                                   as_path  origin_as  \\\n",
      "3       2001:500:2f::/48                6720 1853 13335 13335 3557       3557   \n",
      "4       2001:500:2f::/48                    35369 13335 13335 3557       3557   \n",
      "110    2400:cb00:36::/48   6720 8447 6939 13335 13335 13335 132892     132892   \n",
      "111    2400:cb00:36::/48             6939 13335 13335 13335 132892     132892   \n",
      "112    2400:cb00:36::/48         8596 286 13335 13335 13335 132892     132892   \n",
      "...                  ...                                       ...        ...   \n",
      "47572    103.21.247.0/24  20080 2914 1299 13335 13335 13335 132892     132892   \n",
      "48115    162.158.64.0/21       25152 3257 13335 13335 13335 132892     132892   \n",
      "48116    162.158.64.0/21  20080 2914 3257 13335 13335 13335 132892     132892   \n",
      "48117    162.158.64.0/22       25152 3257 13335 13335 13335 132892     132892   \n",
      "48118    162.158.64.0/22  20080 2914 1299 13335 13335 13335 132892     132892   \n",
      "\n",
      "       provider  pfx_len ip_version  \n",
      "3       13335.0       48       IPv6  \n",
      "4       13335.0       48       IPv6  \n",
      "110     13335.0       48       IPv6  \n",
      "111     13335.0       48       IPv6  \n",
      "112     13335.0       48       IPv6  \n",
      "...         ...      ...        ...  \n",
      "47572   13335.0       24       IPv4  \n",
      "48115   13335.0       21       IPv4  \n",
      "48116   13335.0       21       IPv4  \n",
      "48117   13335.0       22       IPv4  \n",
      "48118   13335.0       22       IPv4  \n",
      "\n",
      "[763 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 prefix                                           as_path  \\\n",
      "34         2.59.98.0/24                                41666 13335 209167   \n",
      "35         2.59.98.0/24                         208594 12735 13335 209167   \n",
      "36         2.59.98.0/24                                13058 13335 209167   \n",
      "37         2.59.98.0/24                                60539 13335 209167   \n",
      "38         2.59.98.0/24  60539 61302 61302 61302 61302 61302 13335 209167   \n",
      "...                 ...                                               ...   \n",
      "1991539  23.29.145.0/24                                 3741 13335 394303   \n",
      "1991540  23.29.145.0/24                        3561 209 3356 13335 394303   \n",
      "1991541  23.29.145.0/24                                34224 13335 394303   \n",
      "1991542  23.29.145.0/24                           20130 6939 13335 394303   \n",
      "1991543  23.29.145.0/24                                11686 13335 394303   \n",
      "\n",
      "         origin_as  provider  pfx_len ip_version  \n",
      "34          209167   13335.0       24       IPv4  \n",
      "35          209167   13335.0       24       IPv4  \n",
      "36          209167   13335.0       24       IPv4  \n",
      "37          209167   13335.0       24       IPv4  \n",
      "38          209167   13335.0       24       IPv4  \n",
      "...            ...       ...      ...        ...  \n",
      "1991539     394303   13335.0       24       IPv4  \n",
      "1991540     394303   13335.0       24       IPv4  \n",
      "1991541     394303   13335.0       24       IPv4  \n",
      "1991542     394303   13335.0       24       IPv4  \n",
      "1991543     394303   13335.0       24       IPv4  \n",
      "\n",
      "[982602 rows x 6 columns]\n",
      "                 prefix                                   as_path  origin_as  \\\n",
      "0       103.21.246.0/24                   32709 6939 13335 132892     132892   \n",
      "1       103.21.246.0/24            53828 13335 13335 13335 132892     132892   \n",
      "2       103.21.246.0/24              852 13335 13335 13335 132892     132892   \n",
      "3       103.21.246.0/24  19653 3356 6453 13335 13335 13335 132892     132892   \n",
      "4       103.21.246.0/24  19016 3257 4436 13335 13335 13335 132892     132892   \n",
      "...                 ...                                       ...        ...   \n",
      "35551  162.158.204.0/24              54073 2044 6939 13335 132892     132892   \n",
      "36024    185.122.0.0/24                     1798 174 13335 203898     203898   \n",
      "36025    185.122.0.0/24                6423 209 6453 13335 203898     203898   \n",
      "36026    185.122.0.0/24              54073 2044 6939 13335 203898     203898   \n",
      "36027    185.122.0.0/24                   15301 6939 13335 203898     203898   \n",
      "\n",
      "       provider  pfx_len ip_version  \n",
      "0       13335.0       24       IPv4  \n",
      "1       13335.0       24       IPv4  \n",
      "2       13335.0       24       IPv4  \n",
      "3       13335.0       24       IPv4  \n",
      "4       13335.0       24       IPv4  \n",
      "...         ...      ...        ...  \n",
      "35551   13335.0       24       IPv4  \n",
      "36024   13335.0       24       IPv4  \n",
      "36025   13335.0       24       IPv4  \n",
      "36026   13335.0       24       IPv4  \n",
      "36027   13335.0       24       IPv4  \n",
      "\n",
      "[505 rows x 6 columns]\n",
      "                  prefix                                   as_path  origin_as  \\\n",
      "3       2001:500:2f::/48                6720 1853 13335 13335 3557       3557   \n",
      "4       2001:500:2f::/48                    35369 13335 13335 3557       3557   \n",
      "110    2400:cb00:36::/48   6720 8447 6939 13335 13335 13335 132892     132892   \n",
      "111    2400:cb00:36::/48             6939 13335 13335 13335 132892     132892   \n",
      "112    2400:cb00:36::/48         8596 286 13335 13335 13335 132892     132892   \n",
      "...                  ...                                       ...        ...   \n",
      "47572    103.21.247.0/24  20080 2914 1299 13335 13335 13335 132892     132892   \n",
      "48115    162.158.64.0/21       25152 3257 13335 13335 13335 132892     132892   \n",
      "48116    162.158.64.0/21  20080 2914 3257 13335 13335 13335 132892     132892   \n",
      "48117    162.158.64.0/22       25152 3257 13335 13335 13335 132892     132892   \n",
      "48118    162.158.64.0/22  20080 2914 1299 13335 13335 13335 132892     132892   \n",
      "\n",
      "       provider  pfx_len ip_version  \n",
      "3       13335.0       48       IPv6  \n",
      "4       13335.0       48       IPv6  \n",
      "110     13335.0       48       IPv6  \n",
      "111     13335.0       48       IPv6  \n",
      "112     13335.0       48       IPv6  \n",
      "...         ...      ...        ...  \n",
      "47572   13335.0       24       IPv4  \n",
      "48115   13335.0       21       IPv4  \n",
      "48116   13335.0       21       IPv4  \n",
      "48117   13335.0       22       IPv4  \n",
      "48118   13335.0       22       IPv4  \n",
      "\n",
      "[763 rows x 6 columns]\n",
      "                 prefix                        as_path  origin_as  provider  \\\n",
      "18        5.181.28.0/24              2914 13335 209242     209242   13335.0   \n",
      "19        5.181.28.0/24             14061 13335 209242     209242   13335.0   \n",
      "20        5.181.28.0/24             19151 13335 209242     209242   13335.0   \n",
      "21        5.181.28.0/24         7575 4826 13335 209242     209242   13335.0   \n",
      "22        5.181.28.0/24             14907 13335 209242     209242   13335.0   \n",
      "...                 ...                            ...        ...       ...   \n",
      "646965  12.221.133.0/24         2152 3356 13335 209242     209242   13335.0   \n",
      "646966  12.221.133.0/24             11686 13335 209242     209242   13335.0   \n",
      "646967  12.221.133.0/24  54728 20130 6939 13335 209242     209242   13335.0   \n",
      "646968  12.221.133.0/24          701 2914 13335 209242     209242   13335.0   \n",
      "646969  12.221.133.0/24              3303 13335 209242     209242   13335.0   \n",
      "\n",
      "        pfx_len ip_version  \n",
      "18           24       IPv4  \n",
      "19           24       IPv4  \n",
      "20           24       IPv4  \n",
      "21           24       IPv4  \n",
      "22           24       IPv4  \n",
      "...         ...        ...  \n",
      "646965       24       IPv4  \n",
      "646966       24       IPv4  \n",
      "646967       24       IPv4  \n",
      "646968       24       IPv4  \n",
      "646969       24       IPv4  \n",
      "\n",
      "[180729 rows x 6 columns]\n",
      "                  prefix                 as_path  origin_as  provider  \\\n",
      "36         5.101.36.0/24       7713 13335 209242     209242   13335.0   \n",
      "37         5.101.36.0/24      38880 13335 209242     209242   13335.0   \n",
      "38         5.101.36.0/24     132337 13335 209242     209242   13335.0   \n",
      "39         5.101.36.0/24       3257 13335 209242     209242   13335.0   \n",
      "40         5.101.36.0/24  9268 3257 13335 209242     209242   13335.0   \n",
      "...                  ...                     ...        ...       ...   \n",
      "1009437  12.236.218.0/24  20130 6939 13335 36406      36406   13335.0   \n",
      "1009438  12.236.218.0/24        2152 13335 36406      36406   13335.0   \n",
      "1009439  12.236.218.0/24       53767 13335 36406      36406   13335.0   \n",
      "1009440  12.236.218.0/24        5413 13335 36406      36406   13335.0   \n",
      "1009441  12.236.218.0/24       11686 13335 36406      36406   13335.0   \n",
      "\n",
      "         pfx_len ip_version  \n",
      "36            24       IPv4  \n",
      "37            24       IPv4  \n",
      "38            24       IPv4  \n",
      "39            24       IPv4  \n",
      "40            24       IPv4  \n",
      "...          ...        ...  \n",
      "1009437       24       IPv4  \n",
      "1009438       24       IPv4  \n",
      "1009439       24       IPv4  \n",
      "1009440       24       IPv4  \n",
      "1009441       24       IPv4  \n",
      "\n",
      "[415648 rows x 6 columns]\n",
      "                 prefix                        as_path  origin_as  provider  \\\n",
      "18        5.181.28.0/24              2914 13335 209242     209242   13335.0   \n",
      "19        5.181.28.0/24             14061 13335 209242     209242   13335.0   \n",
      "20        5.181.28.0/24             19151 13335 209242     209242   13335.0   \n",
      "21        5.181.28.0/24         7575 4826 13335 209242     209242   13335.0   \n",
      "22        5.181.28.0/24             14907 13335 209242     209242   13335.0   \n",
      "...                 ...                            ...        ...       ...   \n",
      "646965  12.221.133.0/24         2152 3356 13335 209242     209242   13335.0   \n",
      "646966  12.221.133.0/24             11686 13335 209242     209242   13335.0   \n",
      "646967  12.221.133.0/24  54728 20130 6939 13335 209242     209242   13335.0   \n",
      "646968  12.221.133.0/24          701 2914 13335 209242     209242   13335.0   \n",
      "646969  12.221.133.0/24              3303 13335 209242     209242   13335.0   \n",
      "\n",
      "        pfx_len ip_version  \n",
      "18           24       IPv4  \n",
      "19           24       IPv4  \n",
      "20           24       IPv4  \n",
      "21           24       IPv4  \n",
      "22           24       IPv4  \n",
      "...         ...        ...  \n",
      "646965       24       IPv4  \n",
      "646966       24       IPv4  \n",
      "646967       24       IPv4  \n",
      "646968       24       IPv4  \n",
      "646969       24       IPv4  \n",
      "\n",
      "[180729 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  prefix                 as_path  origin_as  provider  \\\n",
      "36         5.101.36.0/24       7713 13335 209242     209242   13335.0   \n",
      "37         5.101.36.0/24      38880 13335 209242     209242   13335.0   \n",
      "38         5.101.36.0/24     132337 13335 209242     209242   13335.0   \n",
      "39         5.101.36.0/24       3257 13335 209242     209242   13335.0   \n",
      "40         5.101.36.0/24  9268 3257 13335 209242     209242   13335.0   \n",
      "...                  ...                     ...        ...       ...   \n",
      "1009437  12.236.218.0/24  20130 6939 13335 36406      36406   13335.0   \n",
      "1009438  12.236.218.0/24        2152 13335 36406      36406   13335.0   \n",
      "1009439  12.236.218.0/24       53767 13335 36406      36406   13335.0   \n",
      "1009440  12.236.218.0/24        5413 13335 36406      36406   13335.0   \n",
      "1009441  12.236.218.0/24       11686 13335 36406      36406   13335.0   \n",
      "\n",
      "         pfx_len ip_version  \n",
      "36            24       IPv4  \n",
      "37            24       IPv4  \n",
      "38            24       IPv4  \n",
      "39            24       IPv4  \n",
      "40            24       IPv4  \n",
      "...          ...        ...  \n",
      "1009437       24       IPv4  \n",
      "1009438       24       IPv4  \n",
      "1009439       24       IPv4  \n",
      "1009440       24       IPv4  \n",
      "1009441       24       IPv4  \n",
      "\n",
      "[415648 rows x 6 columns]\n",
      "                  prefix               as_path  origin_as  provider  pfx_len  \\\n",
      "1095    2001:500:2f::/48      14907 13335 3557       3557   13335.0       48   \n",
      "1096    2001:500:2f::/48       6762 13335 3557       3557   13335.0       48   \n",
      "1097    2001:500:2f::/48      19151 13335 3557       3557   13335.0       48   \n",
      "1098    2001:500:2f::/48  7575 4826 13335 3557       3557   13335.0       48   \n",
      "1099    2001:500:a8::/48      6762 13335 21556      21556   13335.0       48   \n",
      "...                  ...                   ...        ...       ...      ...   \n",
      "344658    203.29.53.0/24    43578 13335 209242     209242   13335.0       24   \n",
      "344659   209.141.38.0/23     29680 13335 53667      53667   13335.0       23   \n",
      "344660   209.141.38.0/23     43578 13335 53667      53667   13335.0       23   \n",
      "344661   216.127.76.0/24     29680 13335 36351      36351   13335.0       24   \n",
      "344662   216.127.76.0/24     43578 13335 36351      36351   13335.0       24   \n",
      "\n",
      "       ip_version  \n",
      "1095         IPv6  \n",
      "1096         IPv6  \n",
      "1097         IPv6  \n",
      "1098         IPv6  \n",
      "1099         IPv6  \n",
      "...           ...  \n",
      "344658       IPv4  \n",
      "344659       IPv4  \n",
      "344660       IPv4  \n",
      "344661       IPv4  \n",
      "344662       IPv4  \n",
      "\n",
      "[10544 rows x 6 columns]\n",
      "Step 4 completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Removing records with scrubber as origin.\")\n",
    "# Read csv file and remove AS sets\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "year = \"2024\" \n",
    "scrubber_asn = \"13335\" # Scrubber asn\n",
    "path = \"/data/shared_dir/ddos_scrubber/as\"+scrubber_asn+\"/\"+year+\"/\"\n",
    "\n",
    "pattern = r\"unique_optimized_raw_as\"+scrubber_asn+\".*\\.csv$\"  # Match files with format raw_as32787_*.txt\n",
    "       \n",
    "# Loop through the txt files that contain RIB snapshots of every month  \n",
    "for filename in os.listdir(path):\n",
    "     # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[4] # Get day from file name\n",
    "        mon = filename.split('_')[5] # Get month name from file name\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "        date = year + \"-\" + mon + \"-\" + day\n",
    "\n",
    "        df = pd.read_csv(path + filename, low_memory=False))\n",
    "        \n",
    "        df = pd.read_csv(path + 'unique_optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv', low_memory=False)\n",
    "        origin_scrubber_df = df[df[\"origin_as\"] == int(scrubber_asn)]\n",
    "        origin_scrubber_df.to_csv(path + 'unique_optimized_raw_origin_scrubber_as'+scrubber_asn+'_01_'+mon+_+year+'.csv')\n",
    "        origin_not_scrubber_df = df[df[\"origin_as\"] != int(scrubber_asn)]\n",
    "        origin_not_scrubber_df.to_csv(path + 'unique_optimized_raw_as'+scrubber_asn+'_01_'+mon+'_'+year+'.csv')\n",
    "\n",
    "print(\"Step 5: completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97d606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyrank API\n",
    "# Method to get AS rank of all ASes from CAIDA AS rank API\n",
    "# This part of the code is used from https://github.com/bgpkit/pyasrank\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "ASRANK_ENDPOINT = \"https://api.asrank.caida.org/v2/graphql\"\n",
    "\n",
    "\n",
    "def ts_to_date_str(ts):\n",
    "    \"\"\"\n",
    "    Convert timestamp to a date. This is used for ASRank API which only takes\n",
    "    date strings with no time as parameters.api\n",
    "    \"\"\"\n",
    "    return datetime.utcfromtimestamp(int(ts)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "class AsRank:\n",
    "    \"\"\"\n",
    "    Utilities for using ASRank services\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_ts=\"\"):\n",
    "        self.data_ts = None\n",
    "\n",
    "        # various caches to avoid duplicate queries\n",
    "        self.cache = None\n",
    "        self.cone_cache = None\n",
    "        self.neighbors_cache = None\n",
    "        self.siblings_cache = None\n",
    "        self.organization_cache = None\n",
    "\n",
    "        self.queries_sent = 0\n",
    "\n",
    "        self.session = None\n",
    "        self._initialize_session()\n",
    "\n",
    "        self.init_cache(max_ts)\n",
    "\n",
    "    def _initialize_session(self):\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=1,\n",
    "                        status_forcelist=[500, 502, 503, 504])\n",
    "        self.session.mount(ASRANK_ENDPOINT, HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    def _close_session(self):\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "\n",
    "    def _send_request(self, query):\n",
    "        \"\"\"\n",
    "        send requests to ASRank endpoint\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        r = self.session.post(url=ASRANK_ENDPOINT, json={'query': query})\n",
    "        r.raise_for_status()\n",
    "        self.queries_sent += 1\n",
    "        return r\n",
    "\n",
    "    def init_cache(self, ts):\n",
    "        \"\"\"\n",
    "        Initialize the ASRank cache for the timestamp ts\n",
    "        :param ts:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.cone_cache = {}\n",
    "        self.neighbors_cache = {}\n",
    "        self.siblings_cache = {}\n",
    "        self.organization_cache = {}\n",
    "        self.queries_sent = 0\n",
    "        if isinstance(ts, int):\n",
    "            ts = ts_to_date_str(ts)\n",
    "\n",
    "        ####\n",
    "        # Try to cache datasets available before the given ts\n",
    "        ####\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              datasets(dateStart:\"2000-01-01\", dateEnd:\"%s\", sort:\"-date\", first:1){\n",
    "                edges {\n",
    "                  node {\n",
    "                    date\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % ts\n",
    "        r = self._send_request(graphql_query)\n",
    "\n",
    "        edges = r.json()['data']['datasets']['edges']\n",
    "        if edges:\n",
    "            self.data_ts = edges[0][\"node\"][\"date\"]\n",
    "            return\n",
    "\n",
    "        # if code reaches here, we have not found any datasets before ts. we should now try to find one after ts.\n",
    "        # this is the best effort results\n",
    "        logging.warning(\"cannot find dataset before date %s, looking for the closest one after it now\" % ts)\n",
    "\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              datasets(dateStart:\"%s\", sort:\"date\", first:1){\n",
    "                edges {\n",
    "                  node {\n",
    "                    date\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % ts\n",
    "        r = self._send_request(graphql_query)\n",
    "        edges = r.json()['data']['datasets']['edges']\n",
    "        if edges:\n",
    "            self.data_ts = edges[0][\"node\"][\"date\"]\n",
    "            logging.warning(\"found closest dataset date to be %s\" % self.data_ts)\n",
    "            return\n",
    "        else:\n",
    "            raise ValueError(\"no datasets from ASRank available to use for tagging\")\n",
    "\n",
    "    def _query_asrank_for_asns(self, asns, chunk_size=100):\n",
    "        asns = [str(asn) for asn in asns]\n",
    "        asns_needed = [asn for asn in asns if asn not in self.cache]\n",
    "        if not asns_needed:\n",
    "            return\n",
    "\n",
    "        # https://stackoverflow.com/a/312464/768793\n",
    "        def chunks(lst, n):\n",
    "            \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "            for i in range(0, len(lst), n):\n",
    "                yield lst[i:i + n]\n",
    "\n",
    "        for asns in chunks(asns_needed, chunk_size):\n",
    "\n",
    "            graphql_query = \"\"\"\n",
    "                {\n",
    "                  asns(asns: %s, dateStart: \"%s\", dateEnd: \"%s\", first:%d, sort:\"-date\") {\n",
    "                    edges {\n",
    "                      node {\n",
    "                        date\n",
    "                        asn\n",
    "                        asnName\n",
    "                        rank\n",
    "                        organization{\n",
    "                          country{\n",
    "                            iso\n",
    "                            name\n",
    "                          }\n",
    "                          orgName\n",
    "                          orgId\n",
    "                        } asnDegree {\n",
    "                          provider\n",
    "                          peer\n",
    "                          customer\n",
    "                          total\n",
    "                          transit\n",
    "                          sibling\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "            \"\"\" % (json.dumps(asns), self.data_ts, self.data_ts, len(asns))\n",
    "            r = self._send_request(graphql_query)\n",
    "            try:\n",
    "                for node in r.json()['data']['asns']['edges']:\n",
    "                    data = node['node']\n",
    "                    if data['asn'] not in self.cache:\n",
    "                        if \"asnDegree\" in data:\n",
    "                            degree = data[\"asnDegree\"]\n",
    "                            degree[\"provider\"] = degree[\"provider\"] or 0\n",
    "                            degree[\"customer\"] = degree[\"customer\"] or 0\n",
    "                            degree[\"peer\"] = degree[\"peer\"] or 0\n",
    "                            degree[\"sibling\"] = degree[\"sibling\"] or 0\n",
    "                            data[\"asnDegree\"] = degree\n",
    "                        self.cache[data['asn']] = data\n",
    "                for asn in asns:\n",
    "                    if asn not in self.cache:\n",
    "                        self.cache[asn] = None\n",
    "            except KeyError as e:\n",
    "                logging.error(\"Error in node: {}\".format(r.json()))\n",
    "                logging.error(\"Request: {}\".format(graphql_query))\n",
    "                raise e\n",
    "\n",
    "    ##########\n",
    "    # AS_ORG #\n",
    "    ##########\n",
    "\n",
    "    def are_siblings(self, asn1, asn2):\n",
    "        \"\"\"\n",
    "        Check if two ASes are sibling ASes, i.e. belonging to the same organization\n",
    "        :param asn1: first asn\n",
    "        :param asn2: second asn\n",
    "        :return: True if asn1 and asn2 belongs to the same organization\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn1, asn2])\n",
    "        if any([self.cache[asn] is None for asn in [asn1, asn2]]):\n",
    "            return False\n",
    "        try:\n",
    "            return self.cache[asn1][\"organization\"][\"orgId\"] == self.cache[asn2][\"organization\"][\"orgId\"]\n",
    "        except TypeError:\n",
    "            # we have None for some of the values\n",
    "            return False\n",
    "\n",
    "    def get_organization(self, asn):\n",
    "        \"\"\"\n",
    "        Keys:\n",
    "        - country\n",
    "        - orgName\n",
    "        - orgId\n",
    "\n",
    "        Example return value:\n",
    "        {'country': {'iso': 'US', 'name': 'United States'}, 'orgName': 'Google LLC', 'orgId': 'f7b8c6de69'}\n",
    "\n",
    "        :param asn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn])\n",
    "        if self.cache[asn] is None:\n",
    "            return None\n",
    "        return self.cache[asn][\"organization\"]\n",
    "\n",
    "    \n",
    "\n",
    "    ###########\n",
    "    # AS_RANK #\n",
    "    ###########\n",
    "\n",
    "    def get_degree(self, asn):\n",
    "        \"\"\"\n",
    "        Get relationship summary for asn, including number of customers, providers, peers, etc.\n",
    "\n",
    "        Example return dictionary:\n",
    "        {\n",
    "            \"provider\": 0,\n",
    "            \"peer\": 31,\n",
    "            \"customer\": 1355,\n",
    "            \"total\": 1386,\n",
    "            \"transit\": 1318,\n",
    "            \"sibling\": 25\n",
    "        }\n",
    "        :param asn:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns([asn])\n",
    "        if self.cache[asn] is None:\n",
    "            return None\n",
    "\n",
    "        return self.cache[asn][\"asnDegree\"]\n",
    "\n",
    "    def is_sole_provider(self, asn_pro, asn_cust):\n",
    "        \"\"\"\n",
    "        Verifies if asn_pro and asn_cust are in a customer provider relationship\n",
    "        and asn_pro is the sole upstream of asn_cust (no other providers nor peers\n",
    "        are available to asn_cust).\n",
    "\n",
    "        This function is ported from dataconcierge.ASRank.check_single_upstream. The name of which is confusing, thus\n",
    "        renamed to is_sole_provider.\n",
    "\n",
    "        :param asn_pro: provider ASn (string)\n",
    "        :param asn_cust: ASn in customer cone (string)\n",
    "        :return: True or False\n",
    "        \"\"\"\n",
    "        asn_cust_degree = self.get_degree(asn_cust)\n",
    "        if asn_cust_degree is None:\n",
    "            # missing data for asn_cust\n",
    "            return False\n",
    "        if asn_cust_degree[\"provider\"] == 1 and asn_cust_degree[\"peer\"] == 0 and \\\n",
    "                self.get_relationship(asn_pro, asn_cust) == \"p-c\":\n",
    "            # asn_cust has one provider, no peer, and the provider is asn_pro\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_relationship(self, asn0, asn1):\n",
    "        \"\"\"\n",
    "        Get the AS relationship between asn0 and asn1.\n",
    "\n",
    "        asn0 is asn1's:\n",
    "        - provider: \"p-c\"\n",
    "        - customer: \"c-p\"\n",
    "        - peer: \"p-p\"\n",
    "        - other: None\n",
    "\n",
    "        :param asn0:\n",
    "        :param asn1:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        graphql_query = \"\"\"\n",
    "            {\n",
    "              asnLink(asn0:\"%s\", asn1:\"%s\", date:\"%s\"){\n",
    "              relationship\n",
    "              }\n",
    "            }\n",
    "        \"\"\" % (asn0, asn1, self.data_ts)\n",
    "        r = self._send_request(graphql_query)\n",
    "        if r.json()[\"data\"][\"asnLink\"] is None:\n",
    "            return None\n",
    "        rel = r.json()[\"data\"][\"asnLink\"].get(\"relationship\", \"\")\n",
    "\n",
    "        if rel == \"provider\":\n",
    "            # asn1 is the provider of asn0\n",
    "            return \"c-p\"\n",
    "\n",
    "        if rel == \"customer\":\n",
    "            # asn1 is the customer of asn0\n",
    "            return \"p-c\"\n",
    "\n",
    "        if rel == \"peer\":\n",
    "            # asn1 is the peer of asn0\n",
    "            return \"p-p\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    def in_customer_cone(self, asn0, asn1):\n",
    "        \"\"\"\n",
    "        Check if asn0 is in the customer cone of asn1\n",
    "        :param asn0:\n",
    "        :param asn1:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if asn1 in self.cone_cache:\n",
    "            return asn0 in self.cone_cache[asn1]\n",
    "\n",
    "        graphql_query = \"\"\"\n",
    "        {\n",
    "          asnCone(asn:\"%s\", date:\"%s\"){\n",
    "            asns {\n",
    "              edges {\n",
    "                node {\n",
    "                  asn\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\" % (asn1, self.data_ts)\n",
    "        r = self._send_request(graphql_query)\n",
    "        data = r.json()[\"data\"][\"asnCone\"]\n",
    "        if data is None:\n",
    "            return False\n",
    "        asns_in_cone = {node[\"node\"][\"asn\"] for node in data[\"asns\"][\"edges\"]}\n",
    "        self.cone_cache[asn1] = asns_in_cone\n",
    "        return asn0 in asns_in_cone\n",
    "\n",
    "    def cache_asrank_chunk(self, asns: list, chunk_size: int):\n",
    "        \"\"\"\n",
    "        Query asrank info in chunk to boost individual asrank queries performance later.\n",
    "\n",
    "        :param asns:\n",
    "        :param chunk_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._query_asrank_for_asns(asns, chunk_size)\n",
    "\n",
    "    def get_all_siblings_list(self, asns, chunk_size=100):\n",
    "        self._query_asrank_for_asns(asns, chunk_size)\n",
    "        res = {}\n",
    "        for asn in asns:\n",
    "            res[asn] = self.get_all_siblings(asn)\n",
    "        return res\n",
    "\n",
    "    def get_all_siblings(self, asn, skip_asrank_call=False):\n",
    "        \"\"\"\n",
    "        get all siblings for an ASN\n",
    "        :param asn: AS number to query for all siblings\n",
    "        :param skip_asrank_call: skip asrank call if already done\n",
    "        :return: a tuple of (TOTAL_COUNT, ASNs)\n",
    "        \"\"\"\n",
    "        # FIXME: pagination does not work here. Example ASN5313.\n",
    "        asn = str(asn)\n",
    "        if asn in self.siblings_cache:\n",
    "            return self.siblings_cache[asn]\n",
    "\n",
    "        if not skip_asrank_call:\n",
    "            self._query_asrank_for_asns([asn])\n",
    "\n",
    "        if asn not in self.cache or self.cache[asn] is None:\n",
    "            return 0, []\n",
    "        asrank_info = self.cache[asn]\n",
    "        if \"organization\" not in asrank_info or asrank_info[\"organization\"] is None:\n",
    "            return 0, []\n",
    "\n",
    "        org_id = self.cache[asn][\"organization\"][\"orgId\"]\n",
    "\n",
    "        if org_id in self.organization_cache:\n",
    "            data = self.organization_cache[org_id]\n",
    "        else:\n",
    "            graphql_query = \"\"\"\n",
    "            {\n",
    "            organization(orgId:\"%s\"){\n",
    "              orgId,\n",
    "              orgName,\n",
    "              members{\n",
    "                numberAsns,\n",
    "                numberAsnsSeen,\n",
    "                asns{totalCount,edges{node{asn,asnName}}}\n",
    "              }\n",
    "            }}        \n",
    "            \"\"\" % org_id\n",
    "            r = self._send_request(graphql_query)\n",
    "            data = r.json()[\"data\"][\"organization\"]\n",
    "            self.organization_cache[org_id] = data\n",
    "\n",
    "        if data is None:\n",
    "            return 0, []\n",
    "\n",
    "        total_cnt = data[\"members\"][\"asns\"][\"totalCount\"]\n",
    "        siblings = set()\n",
    "        for sibling_data in data[\"members\"][\"asns\"][\"edges\"]:\n",
    "            siblings.add(sibling_data[\"node\"][\"asn\"])\n",
    "        if asn in siblings:\n",
    "            siblings.remove(asn)\n",
    "            total_cnt -= 1\n",
    "\n",
    "        # NOTE: this assert can be wrong when number of siblings needs pagination\n",
    "        # assert len(siblings) == total_cnt - 1\n",
    "\n",
    "        siblings = list(siblings)\n",
    "        self.neighbors_cache[asn] = (total_cnt, siblings)\n",
    "        return total_cnt, siblings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32a85545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14789', '395747', '394536', '13335', '209242']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_rank = AsRank(\"2023-dec-31\")\n",
    "b = as_rank.get_all_siblings(\"13335\")[1]\n",
    "b.append(\"13335\")\n",
    "b.append( \"209242\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da9b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siblings of ASN 32787: ['12222', '16625', '16702', '17204', '17334', '18680', '18717', '20189', '22207', '22452', '23454', '23455', '26008', '30675', '31984', '33047', '35993', '35994', '36029', '36183', '393234', '393560']\n"
     ]
    }
   ],
   "source": [
    "# Program to find siblings of an ASN based on CAIDA AS2Org data\n",
    "# Read line from 95721 from the file /h # format:aut|changed|aut_name|org_id|opaque_id|source\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# Function to find siblings of a given ASN in a .jsonl.gz file, starting from a specific line\n",
    "def find_siblings(asn_to_find):\n",
    "    # Create a dictionary to group ASNs by organizationId\n",
    "    org_id_map = {}\n",
    "    \n",
    "    file_path = path +'20241001.as-org2info.jsonl.gz' # Used this because AS Rank API was down.\n",
    "\n",
    "    start_line = 95721\n",
    "        \n",
    "    # Open and read the .jsonl.gz file\n",
    "    with gzip.open(file_path, 'rt') as f:  # 'rt' is for reading text mode\n",
    "        for current_line, line in enumerate(f):\n",
    "            if current_line < start_line:\n",
    "                continue  # Skip lines until reaching the desired start_line\n",
    "            \n",
    "            record = json.loads(line)  # Parse each line as JSON\n",
    "            \n",
    "            asn = record['asn']\n",
    "            organization_id = record['organizationId']\n",
    "            \n",
    "            if organization_id not in org_id_map:\n",
    "                org_id_map[organization_id] = []\n",
    "            org_id_map[organization_id].append(asn)\n",
    "    \n",
    "    # Find the organizationId of the given ASN\n",
    "    org_id_of_asn = None\n",
    "    for org_id, asns in org_id_map.items():\n",
    "        if asn_to_find in asns:\n",
    "            org_id_of_asn = org_id\n",
    "            break\n",
    "\n",
    "    # If ASN is not found, return an empty list\n",
    "    if org_id_of_asn is None:\n",
    "        return []\n",
    "\n",
    "    # Return all ASNs that share the same organizationId, excluding the given ASN\n",
    "    siblings = [asn for asn in org_id_map[org_id_of_asn] if asn != asn_to_find]\n",
    "    return siblings\n",
    "\n",
    "# Test the function with a .jsonl.gz file, starting from line 10\n",
    "asn_to_find = scrubber_asn\n",
    "siblings = find_siblings(asn_to_find)\n",
    "print(f\"Siblings of ASN {asn_to_find}: {siblings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a17b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Finding records that were not originated by the scrubber and contain scrubber as the second last hop.\n",
      "Calling AS Rank API.\n",
      "Step 5 completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Finding records that were not originated by the scrubber and contain scrubber as the second last hop.\")\n",
    "\n",
    "pattern = r\"unique_optimized_raw_as13335_01_feb_2024.*\\.csv$\"  # Match files with format unique_optimized_raw_as32787_*.csv\n",
    "path = \"../data/\"\n",
    "scrubber_asn = \"13335\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Loop through all the .csv files\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[4] # Get day from file name\n",
    "        mon = filename.split('_')[5] # Get month name from file name\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "        date = year + \"-\" + mon + \"-\" + day\n",
    "\n",
    "        df = pd.read_csv(path + filename, low_memory=False)\n",
    "\n",
    "        # Find number of unique prefixes that do not contain scrubber as well as its siblings as origin\n",
    "        # Siblings are found using ASRank API\n",
    "        print(\"Calling AS Rank API.\")\n",
    "        as_rank = AsRank(date)\n",
    "#         print(\"Wait until 30 seconds.\")\n",
    "#         time.sleep(30) # Added because AS rank API threw error sending many requests in a few seconds.\n",
    "        \n",
    "        siblings = as_rank.get_all_siblings(scrubber_asn)[1]\n",
    "        siblings.append(scrubber_asn)\n",
    "        siblings.append(\"209242\") # NOte it is only for cloudflare\n",
    "        \n",
    "        # Convert string list to int list\n",
    "        siblings_int = [int(x) for x in siblings]\n",
    "\n",
    "        condition = (~df['origin_as'].isin(siblings_int)) \n",
    "        df2 = df.loc[condition]\n",
    "        \n",
    "        origin_scrubber_df = df[df[\"origin_as\"] == int(scrubber_asn)]\n",
    "        origin_scrubber_df.to_csv(path + \"unique_optimized_raw_origin_scrubber_as13335_01_feb_2024.csv\")\n",
    "        \n",
    "        # Prefixes containing scrubber asn as the second last hop AS in AS path\n",
    "        provider_scrubber_records = df2.loc[df['provider'] == int(scrubber_asn)]\n",
    "\n",
    "#         # Save confirmed_customers1 to a csv file\n",
    "#         provider_scrubber_records.to_csv(path+\"confirmed_customers_as\"+scrubber_asn+\"_\"+day+\"_\"+mon+\"_\"+year+\".csv\", index=False)\n",
    "#         # Unique origin ASes are actually CONFIRMED customers that have their second last hop AS as AS198949\n",
    "#         unique_origin_ases = provider_scrubber_records['origin_as'].unique()\n",
    "#         print(\"%s number of unique customer ASNs of AS%s on %s\" %(len(unique_origin_ases), scrubber_asn, mon))\n",
    "\n",
    "#         unique_prefixes = provider_scrubber_records['prefix'].unique()\n",
    "#         print(\"%s number of unique prefixes that contain AS%s as provider on %s %s.\\n\" %(len(unique_prefixes), scrubber_asn, day, mon))\n",
    "        \n",
    "#         print(\"Finding cases where second last hop is not the scrubber ASN.\")\n",
    "#         provider_not_scrubber = df2.loc[df['provider'] != int(scrubber_asn)]\n",
    "#         provider_not_scrubber.to_csv(path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_\"+mon+\"_\"+year+\".csv\", index=False)\n",
    "#         print(\"%s number of prefixes do not contain AS%s as a provider (second last hop in AS path).\\n\" %(len(provider_not_scrubber), scrubber_asn))\n",
    "#         del as_rank # Destroy object\n",
    "        \n",
    "print(\"Step 5 completed.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6333b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ASRank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m date \u001b[38;5;241m=\u001b[39m year \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m mon \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m day\n\u001b[1;32m      6\u001b[0m date\n\u001b[0;32m----> 7\u001b[0m ad \u001b[38;5;241m=\u001b[39m \u001b[43mASRank\u001b[49m(date)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ASRank' is not defined"
     ]
    }
   ],
   "source": [
    "a = \"unique_optimized_raw_as13335_01_feb_2023.csv\"\n",
    "day = a.split('_')[4] # Get day from file name\n",
    "mon = a.split('_')[5] # Get month name from file name\n",
    "year = a.split('_')[-1].split('.')[0]\n",
    "date = year + \"-\" + mon + \"-\" + day\n",
    "date\n",
    "ad = ASRank(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c9aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Finding customers doing AS path prepending.\n",
      "66817 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_jul_2022.csv.\n",
      "\n",
      "65920 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_nov_2022.csv.\n",
      "\n",
      "66431 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_dec_2022.csv.\n",
      "\n",
      "45521 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_feb_2022.csv.\n",
      "\n",
      "63591 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_oct_2022.csv.\n",
      "\n",
      "67802 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_sep_2022.csv.\n",
      "\n",
      "47362 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_mar_2022.csv.\n",
      "\n",
      "25216 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_jun_2022.csv.\n",
      "\n",
      "68415 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_aug_2022.csv.\n",
      "\n",
      "66892 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_may_2022.csv.\n",
      "\n",
      "44091 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_jan_2022.csv.\n",
      "\n",
      "48461 number of records have AS path prepended and contain AS32787 as a provider(not a second last hop in AS path though).\n",
      "\n",
      "Records are stored in the file /home/shyam/jupy/ddos_scrubber/data/as32787/2022/unique_optimized_provider_as32787_path_prepend_01_apr_2022.csv.\n",
      "\n",
      "Step 4 completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year = \"2022\"\n",
    "print(\"Step 6: Finding customers doing AS path prepending.\")\n",
    "\n",
    "\n",
    "# Function to check and validate the 'as_path' removing AS set or other strings in AS path\n",
    "def is_valid_as_path(row):\n",
    "    try:\n",
    "        # Try converting the as_path to a list of integers\n",
    "        parts = list(map(int, row['as_path'].split()))\n",
    "        return True  # Return True if valid\n",
    "    except ValueError:\n",
    "        return False  # Return False if invalid\n",
    "\n",
    "            \n",
    "# Define a function to determine path prepending and new_provider\n",
    "def determine_path_prepending_and_new_provider(row):\n",
    "   \n",
    "    # Ensure as_path is a string and split into a list of ASNs\n",
    "#     print(\"AS path is %s\" %row['as_path'])\n",
    "    try:\n",
    "        as_path = list(map(int, str(row['as_path']).split()))\n",
    "        \n",
    "    except ValueError:\n",
    "        # Catch ValueError, print error, and skip the invalid path\n",
    "        return None\n",
    "        \n",
    "    provider = None # Default value set\n",
    "    if len(as_path) < 2:\n",
    "        provider = None  # Not enough ASNs in path to determine provider\n",
    "\n",
    "    origin_as = as_path[-1]  # The last ASN is the origin ASN\n",
    "    \n",
    "    path_prepending = 1 if as_path.count(origin_as) > 1 else 0\n",
    "\n",
    "#     provider = as_path[-2]  # If all checks fail, return the second ASN by default\n",
    "    \n",
    "    # Check for sequentially repeated ASNs\n",
    "    repeated_asn = None\n",
    "    for i in range(len(as_path) - 1, 0, -1):\n",
    "        if as_path[i-1] == as_path[i]:\n",
    "            repeated_asn = as_path[i]\n",
    "        else:\n",
    "            # If we find an ASN that is not the same as the repeated ASN,\n",
    "            # and we have found a repeated ASN, return the one before the repeated ASN\n",
    "            if repeated_asn is not None:\n",
    "                provider = as_path[i-1]  # This is the upstream provider\n",
    "            break\n",
    "    \n",
    "    return pd.Series([path_prepending, provider])\n",
    "\n",
    "pattern = r\"unique_optimized_provider_not_as.*\\.csv$\"  # Match files with format unique_optimized_raw_as32787_*.csv\n",
    "\n",
    "# Loop through all the .csv files\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[5] # Get day from file name\n",
    "        mon = filename.split('_')[6] # Get month name from file name\n",
    "        year = filename.split('_')[-1].split('.')[0]\n",
    "\n",
    "        date = year + \"-\" + mon + \"-\" + day\n",
    "       \n",
    "        df = pd.read_csv(path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_\"+mon+\"_\"+year+\".csv\")\n",
    "        \n",
    "        # Filter the DataFrame to keep only valid rows\n",
    "        df = df[df.apply(is_valid_as_path, axis=1)]\n",
    "        \n",
    "        # Apply the function to each row\n",
    "        df[['path_prepending', 'new_provider']] = df.apply(determine_path_prepending_and_new_provider, axis=1)\n",
    "\n",
    "        # Print the updated DataFrame to verify results\n",
    "        df.to_csv(path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_\"+mon+\"_\"+year+\"_v2.csv\") \n",
    "\n",
    "        # Find the records of AS path prepending\n",
    "        condition = (df['path_prepending'] == 1) & (df['new_provider'] == int(scrubber_asn))\n",
    "        provider_scrubber_as_path_prepend = df.loc[condition]\n",
    "        output_file = path + \"unique_optimized_provider_as\" + scrubber_asn +\"_path_prepend_01_\"+mon+\"_\"+year+\".csv\"\n",
    "        provider_scrubber_as_path_prepend.to_csv(output_file)\n",
    "        print(\"%s number of records have AS path prepended and contain AS%s as a provider(not a second last hop in AS path though).\\n\" %(len(provider_scrubber_as_path_prepend), scrubber_asn))\n",
    "        print(\"Records are stored in the file %s.\\n\" %(output_file))\n",
    "        \n",
    "        # Remove original file that contains duplicate records\n",
    "        os.remove(path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_\"+mon+\"_\"+year+\".csv\")\n",
    "print(\"Step 6: AS path prepending check completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be0cb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shyam/jupy/ddos_scrubber/data/as32787/2022/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaaa22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Removing old files in Aruba machine.\n",
      "\n",
      "Now uploading the files to Aruba machine for checking siblings.\n",
      "\n",
      "Login to Aruba machine with command arb, to check whether all the files are uploaded. \n",
      "Then run a script there with a URL http://127.0.0.1:8900/lab?token=34adaf326a3359566171ad2c5e6e0ff3d9be13cfef9e154a.\n",
      "\n",
      "Step 5 completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Step 5: Removing old files in Aruba machine.\\n\")\n",
    "os.system(\"ssh aruba-shyam 'rm -f /home/shyam/data/workspace/project/notebooks/unique_optimized_provider_not_as*'\")\n",
    "      \n",
    "print(\"Now uploading the files to Aruba machine for checking siblings.\\n\")\n",
    "\n",
    "pattern = r\"unique_optimized_provider_not_as.*\\_v2.csv$\"  # Match files with format unique_optimized_raw_as32787_*.csv\n",
    "\n",
    "# Loop through all the .csv files\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[5] # Get day from file name\n",
    "        mon = filename.split('_')[6] # Get month name from file name\n",
    "        year = filename.split('_')[7] # Get month name from file name\n",
    "        os.system(\"scp \"+path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_\"+mon+\"_\"+year+\"_v2.csv aruba-shyam:/home/shyam/data/workspace/project/notebooks\")\n",
    "\n",
    "print(\"Login to Aruba machine with command arb, to check whether all the files are uploaded. \\nThen run a script there with a URL http://127.0.0.1:8900/lab?token=34adaf326a3359566171ad2c5e6e0ff3d9be13cfef9e154a.\\n\")\n",
    "\n",
    "# Run local script remotely to in the machine\n",
    "# os.system(\"ssh aruba-shyam python3  < aruba_script.py\")\n",
    "\n",
    "# Run python script located remotely \n",
    "# os. system(\"ssh aruba-shyam 'python3 /home/shyam/data/workspace/project/notebooks/check_scrubber_siblings.py'\")\n",
    "print(\"Step 5 completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
