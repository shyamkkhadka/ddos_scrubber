{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program after checking siblings in Aruba machine remotely\n",
    "scrubber_asn = \"32787\" # Scrubber asn\n",
    "year = \"2022\"\n",
    "day = \"01\" # It is default day for our analysis\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "import re\n",
    "\n",
    "path = \"/home/shyam/jupy/ddos_scrubber/data/as\"+scrubber_asn+\"/\"+year+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2235ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading files from Aruba machine.\\n\")\n",
    "os.system(\"scp aruba-shyam:/home/shyam/data/workspace/project/notebooks/unique_optimized_provider_not_as*_v3.csv \"+path)\n",
    "print(\"Downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55647ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 6: Finding customers that have sibling ASes.\\n\")\n",
    "pattern = r\"unique_optimized_provider_not_as\"+scrubber_asn+\".*\\_v3.csv$\" \n",
    "for filename in os.listdir(path):\n",
    "     # Read only processed files\n",
    "    if re.search(pattern, filename):\n",
    "        day = filename.split('_')[5] # Get day from file name\n",
    "        mon = filename.split('_')[6] # Get month name from file name\n",
    "        year = filename.split('_')[7]\n",
    "        \n",
    "        df = pd.read_csv(path + filename)\n",
    "        # Confirmed customers (AS198949 comes immediately after the siblings) after sibling check\n",
    "        confirmed_customers_after_sibling_check = df.loc[(df['siblings'] == 1) & (df['new_provider_sibling_check'] == int(scrubber_asn))]\n",
    "#         print(\"%s number of records have sibling ASes and contain AS%s as a provider (not a second last hop in AS path though)\" %(len(confirmed_customers_after_sibling_check), scrubber_asn))\n",
    "\n",
    "        no_sibling_different_provider_records = df.loc[(df['siblings'] == 0) & (df['new_provider_sibling_check'] != int(scrubber_asn))]\n",
    "#         print(\"%s number of records do not have sibling ASes and contain another provider\" %len(no_sibling_different_provider_records))\n",
    "        no_sibling_different_provider_records\n",
    "print(\"Step 6 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f53f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 7: Finding direct customers, confirmed_customers_after_sibling_check and path prepending.\\n\")\n",
    "path = \"/home/shyam/jupy/ddos_scrubber/data/as\"+scrubber_asn+\"/\"+year+\"/\"\n",
    "\n",
    "months = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "all_record_ases = [] # Stores number of customer ases for a month\n",
    "all_record_prefixes = [] # Stores number of customer prefixes for a month\n",
    "\n",
    "\n",
    "for mon in months:\n",
    "    single_record_ases = [] # Stores number of customer ases for a month\n",
    "    single_record_prefixes = [] # Stores number of customer prefixes for a month\n",
    "\n",
    "    confirmed_customers1 = pd.read_csv(path + \"confirmed_customers_as\"+scrubber_asn+\"_\"+ day +\"_\"+mon+\"_\" +year+\".csv\")\n",
    "    confirmed_customers1_unique_origin_ases = confirmed_customers1['origin_as'].unique()\n",
    "    confirmed_customers1_unique_origin_prefixes = confirmed_customers1['prefix'].unique()\n",
    "    \n",
    "    path_prepending = pd.read_csv(path + \"unique_optimized_provider_as\"+scrubber_asn+\"_path_prepend_01_\"+mon+\"_\"+year+\".csv\")\n",
    "    path_prepending_unique_origin_ases = path_prepending['origin_as'].unique()\n",
    "    \n",
    "    path_prepending_unique_origin_prefixes = path_prepending['prefix'].unique()\n",
    "\n",
    "    confirmed_customers2_ases = list(set(path_prepending_unique_origin_ases)-set(confirmed_customers1_unique_origin_ases))\n",
    "    confirmed_customers2_prefixes = list(set(path_prepending_unique_origin_prefixes)-set(confirmed_customers1_unique_origin_prefixes))\n",
    "\n",
    "    df = pd.read_csv(path + \"unique_optimized_provider_not_as\"+scrubber_asn+\"_01_\"+ mon +\"_\"+year+\"_v3.csv\")\n",
    "    sibling_path = df.loc[(df['siblings'] == 1) & (df['new_provider_sibling_check'] == int(scrubber_asn))]\n",
    "    sibling_path_unique_origin_ases = sibling_path['origin_as'].unique()\n",
    "    sibling_path_unique_origin_prefixes = sibling_path['prefix'].unique()\n",
    "\n",
    "    confirmed_customers3_ases = list(set(sibling_path_unique_origin_ases)-set(confirmed_customers2_ases)-set(confirmed_customers1_unique_origin_ases))\n",
    "    confirmed_customers3_prefixes = list(set(sibling_path_unique_origin_prefixes)-set(confirmed_customers2_prefixes)-set(confirmed_customers1_unique_origin_prefixes))\n",
    "\n",
    "    confirmed_customers_ases = list(confirmed_customers1_unique_origin_ases) + list(confirmed_customers2_ases) + list(confirmed_customers3_ases)\n",
    "    confirmed_customers_prefixes = list(confirmed_customers1_unique_origin_prefixes) + list(confirmed_customers2_prefixes) + list(confirmed_customers3_prefixes)\n",
    "\n",
    "#     confirmed_customers_ases.to_csv(path + \"final_customers_ases_\"+mon+\".csv\", index = False)\n",
    "#     confirmed_customers_prefixes.to_csv(path + \"final_customers_prefixes_\"+mon+\".csv\", index = False)\n",
    "\n",
    "    single_record_ases.append(mon)\n",
    "    single_record_ases.append(year)\n",
    "    single_record_ases.append(len(confirmed_customers_ases))\n",
    "    all_record_ases.append(single_record_ases)\n",
    "\n",
    "    single_record_prefixes.append(mon)\n",
    "    single_record_prefixes.append(year)\n",
    "    single_record_prefixes.append(len(confirmed_customers_prefixes))\n",
    "    all_record_prefixes.append(single_record_prefixes)\n",
    "    \n",
    "    \n",
    "print(\"Step 7 completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 8: Saving customer ASes in the csv file.\\n\")\n",
    "import csv\n",
    "headers = [\"month\", \"year\", \"no_customer_ases\"] \n",
    "\n",
    "filename_ases = path + \"final_confirmed_customer_ases_as\" + scrubber_asn + \"_\"+year+\".csv\"\n",
    "\n",
    "with open(filename_ases, 'a') as f:\n",
    "    write = csv.writer(f)\n",
    "#     write.writerow(headers)\n",
    "    write.writerows(all_record_ases)\n",
    "\n",
    "print(\"Step 8 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18053b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 9: Saving customer prefixes in the csv file.\\n\")\n",
    "import csv\n",
    "\n",
    "# header names \n",
    "headers = [\"month\", \"year\", \"no_customer_prefixes\"] \n",
    "\n",
    "filename_prefixes = path + \"final_confirmed_customer_prefixes_as\" + scrubber_asn + \"_\"+year+\".csv\"\n",
    "\n",
    "with open(filename_prefixes, 'a') as f: \n",
    "    write = csv.writer(f)\n",
    "#     write.writerow(headers) # Remove header after the first file is processed.\n",
    "    write.writerows(all_record_prefixes)\n",
    "\n",
    "print(\"Step 9 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 9: Storing files in the DACS object storage.\")\n",
    "import os\n",
    "os.system(\"mc mb obj-dacs/catrin/data_processing/tool=ddos_scrubber/customer/as\"+ scrubber_asn + \"/\"+year)\n",
    "os.system(\"mc cp -r \"+ path +\"/* obj-dacs/catrin/data_processing/tool=ddos_scrubber/customer/as\"+scrubber_asn + \"/\"+year)\n",
    "print(\"Step 10 completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
